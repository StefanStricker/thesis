{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "78c099d5",
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torchvision import models, transforms, datasets\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from torch.utils.data import DataLoader\n",
        "from pathlib import Path\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "e8250bc1",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "e1eb59ae",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_path1 = Path(\"data/datasets/trashnet_01/\")\n",
        "input_path2 = Path(\"data/datasets/self-collected/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "d970934e",
      "metadata": {},
      "outputs": [],
      "source": [
        "eval_tf = transforms.Compose([\n",
        "    transforms.Resize((224,224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                         std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "\n",
        "source_ds = datasets.ImageFolder(input_path1 / \"test\",  transform= eval_tf)\n",
        "\n",
        "target_ds = datasets.ImageFolder(input_path2, transform= eval_tf)\n",
        "\n",
        "dataloader_source = DataLoader(\n",
        "    source_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "dataloader_target = DataLoader(\n",
        "    target_ds,\n",
        "    batch_size=32,\n",
        "    shuffle=False,\n",
        "    num_workers=4\n",
        ")\n",
        "\n",
        "num_classes = len(target_ds.classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97c8460c",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Function to rebuild models for evaluation\n",
        "\n",
        "def rebuild_model(filename: str, num_classes: int, device):\n",
        "    \n",
        "    if filename.startswith(\"resnet50\"):\n",
        "        m = models.resnet50(weights=None)\n",
        "        m.fc = nn.Linear(m.fc.in_features, num_classes)\n",
        "\n",
        "    elif filename.startswith(\"densenet121\"):\n",
        "        m = models.densenet121(weights=None)\n",
        "        m.classifier = nn.Linear(m.classifier.in_features, num_classes)\n",
        "\n",
        "    elif filename.startswith(\"convnext_tiny\"):\n",
        "        m = models.convnext_tiny(weights=None)\n",
        "        m.classifier[2] = nn.Linear(m.classifier[2].in_features, num_classes)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown architecture in filename: {filename}\")\n",
        "\n",
        "    return m.to(device)                            "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "891f7867",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Function to calculate metrics\n",
        "\n",
        "def evaluate_model_metrics(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds_list, labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            preds_list.extend(preds.cpu().tolist())\n",
        "            labels_list.extend(labels.cpu().tolist())\n",
        "\n",
        "        return {\n",
        "            \"accuracy\": accuracy_score(labels_list, preds_list),\n",
        "            \"macro_precision\": precision_score(labels_list, preds_list, average=\"macro\", zero_division=0),\n",
        "            \"macro_recall\": recall_score(labels_list, preds_list, average=\"macro\", zero_division=0),\n",
        "            \"macro_f1\": f1_score(labels_list, preds_list, average=\"macro\", zero_division=0),\n",
        "            \"n\": len(labels_list),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dc2a377b",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "               model  source_acc  source_macro_precision  source_macro_recall  source_macro_f1  target_acc  target_macro_precision  target_macro_recall  target_macro_f1  source_n  target_n  macro_f1_drop\n",
            "  convnext_tiny_base      0.9377                  0.9361               0.9288           0.9321      0.4812                  0.5977               0.4795           0.4556       257       478         0.4765\n",
            "   convnext_tiny_geo      0.9144                  0.9022               0.9242           0.9087      0.4874                  0.5683               0.4858           0.4691       257       478         0.4396\n",
            " convnext_tiny_mixed      0.9261                  0.9119               0.9132           0.9119      0.4456                  0.5061               0.4444           0.4406       257       478         0.4713\n",
            " convnext_tiny_photo      0.9533                  0.9590               0.9362           0.9451      0.4958                  0.5958               0.4944           0.4889       257       478         0.4562\n",
            "densenet121_baseline      0.9416                  0.9324               0.9345           0.9324      0.3473                  0.5915               0.3466           0.3363       257       478         0.5961\n",
            "     densenet121_geo      0.9261                  0.9078               0.9193           0.9115      0.4603                  0.5564               0.4590           0.4486       257       478         0.4629\n",
            "     densenet121_mix      0.9339                  0.9369               0.9245           0.9294      0.4184                  0.5410               0.4170           0.3974       257       478         0.5320\n",
            "   densenet121_photo      0.9261                  0.9014               0.9117           0.9055      0.4498                  0.5971               0.4492           0.4494       257       478         0.4561\n",
            "   resnet50_baseline      0.9494                  0.9365               0.9308           0.9333      0.5523                  0.6408               0.5507           0.5363       257       478         0.3970\n",
            "        resnet50_geo      0.9339                  0.9182               0.9159           0.9167      0.5230                  0.6044               0.5216           0.5164       257       478         0.4003\n",
            "      resnet50_mixed      0.9494                  0.9331               0.9387           0.9354      0.5795                  0.6697               0.5776           0.5591       257       478         0.3763\n",
            "      resnet50_photo      0.9416                  0.9387               0.9314           0.9348      0.5879                  0.6604               0.5869           0.5902       257       478         0.3446\n"
          ]
        }
      ],
      "source": [
        "#Calculating metrics for all the models\n",
        "\n",
        "cnn_models_dir = \"trained_models\"\n",
        "rows = []\n",
        "\n",
        "for entry in sorted(os.listdir(cnn_models_dir)):\n",
        "    if not entry.endswith(\".pth\"):\n",
        "        continue\n",
        "\n",
        "    ckpt_path = os.path.join(cnn_models_dir, entry)\n",
        "    model_name = entry.replace(\".pth\", \"\")\n",
        "\n",
        "    model = rebuild_model(entry, num_classes, device)\n",
        "\n",
        "    ckpt = torch.load(ckpt_path, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "    src = evaluate_model_metrics(model, dataloader_source, device)\n",
        "    tgt = evaluate_model_metrics(model, dataloader_target, device)\n",
        "\n",
        "    rows.append({\n",
        "        \"model\": model_name,\n",
        "\n",
        "        \"source_acc\": src[\"accuracy\"],\n",
        "        \"source_macro_precision\": src[\"macro_precision\"],\n",
        "        \"source_macro_recall\": src[\"macro_recall\"],\n",
        "        \"source_macro_f1\": src[\"macro_f1\"],\n",
        "\n",
        "        \"target_acc\": tgt[\"accuracy\"],\n",
        "        \"target_macro_precision\": tgt[\"macro_precision\"],\n",
        "        \"target_macro_recall\": tgt[\"macro_recall\"],\n",
        "        \"target_macro_f1\": tgt[\"macro_f1\"],\n",
        "\n",
        "        \"source_n\": src[\"n\"],\n",
        "        \"target_n\": tgt[\"n\"],\n",
        "    })\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "metric_cols = [c for c in df.columns if c not in [\"model\", \"source_n\", \"target_n\"]]\n",
        "df[metric_cols] = df[metric_cols].round(4)\n",
        "\n",
        "df[\"macro_f1_drop\"] = (df[\"source_macro_f1\"] - df[\"target_macro_f1\"]).round(4)\n",
        "\n",
        "print(df.to_string(index=False))    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "472ab67c",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_subgroup_folder(\n",
        "    csv_path,\n",
        "    images_root,\n",
        "    out_root,\n",
        "    filter_col,\n",
        "    filter_value,        \n",
        "    class_col=\"class\",\n",
        "    id_col=\"id\",\n",
        "    overwrite=True,\n",
        "    copy_files=True    \n",
        "):\n",
        "    csv_path = Path(csv_path)\n",
        "    images_root = Path(images_root)\n",
        "    out_root = Path(out_root)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    sub = df[df[filter_col] == filter_value].copy()\n",
        "    subgroup_name = f\"{filter_col}_{filter_value}\"\n",
        "    dst_root = out_root / subgroup_name\n",
        "\n",
        "    if overwrite and dst_root.exists():\n",
        "        shutil.rmtree(dst_root)\n",
        "    dst_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    missing = 0\n",
        "    written = 0\n",
        "\n",
        "    for _, row in sub.iterrows():\n",
        "        cls = str(row[class_col]).strip()\n",
        "        fname = str(row[id_col]).strip()\n",
        "\n",
        "        src = images_root / cls / fname\n",
        "        if not src.exists():\n",
        "            missing += 1\n",
        "            continue\n",
        "\n",
        "        dst_class_dir = dst_root / cls\n",
        "        dst_class_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dst = dst_class_dir / fname\n",
        "\n",
        "        if copy_files:\n",
        "            shutil.copy2(src, dst)\n",
        "        else:\n",
        "            if dst.exists():\n",
        "                dst.unlink()\n",
        "            dst.symlink_to(src.resolve())\n",
        "\n",
        "        written += 1\n",
        "        \n",
        "    return dst_root\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "7ddfb413",
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = \"subgroups.csv\"\n",
        "images_root = input_path2\n",
        "out_root = \"subgroups\"\n",
        "\n",
        "low_light_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"lighting\", filter_value=2,\n",
        "                                       copy_files=True)\n",
        "\n",
        "good_light_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                        filter_col=\"lighting\", filter_value=1,\n",
        "                                        copy_files=True)\n",
        "\n",
        "background_tree_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"tree\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_wall_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"wall\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_white_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"white\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_floor_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"floor\",\n",
        "                                       copy_files=True)  \n",
        "\n",
        "near_view_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"view\", filter_value=1,\n",
        "                                       copy_files=True)\n",
        "\n",
        "far_view_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                        filter_col=\"view\", filter_value=2,\n",
        "                                        copy_files=True)                                                                                                                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a6b4098",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "=== Evaluating model: convnext_tiny_base ===\n",
            "  background_floor     | n= 120 | acc=0.5917 | macroF1=0.5574\n",
            "  background_tree      | n= 120 | acc=0.3417 | macroF1=0.2757\n",
            "  background_wall      | n= 118 | acc=0.4153 | macroF1=0.3827\n",
            "  background_white     | n= 120 | acc=0.5750 | macroF1=0.5508\n",
            "  lighting_1           | n= 239 | acc=0.5439 | macroF1=0.5244\n",
            "  lighting_2           | n= 239 | acc=0.4184 | macroF1=0.3758\n",
            "  view_1               | n= 240 | acc=0.5167 | macroF1=0.4901\n",
            "  view_2               | n= 238 | acc=0.4454 | macroF1=0.4192\n",
            "\n",
            "=== Evaluating model: convnext_tiny_geo ===\n",
            "  background_floor     | n= 120 | acc=0.5917 | macroF1=0.5606\n",
            "  background_tree      | n= 120 | acc=0.3750 | macroF1=0.3559\n",
            "  background_wall      | n= 118 | acc=0.4576 | macroF1=0.4322\n",
            "  background_white     | n= 120 | acc=0.5250 | macroF1=0.5212\n",
            "  lighting_1           | n= 239 | acc=0.5649 | macroF1=0.5532\n",
            "  lighting_2           | n= 239 | acc=0.4100 | macroF1=0.3774\n",
            "  view_1               | n= 240 | acc=0.5250 | macroF1=0.5024\n",
            "  view_2               | n= 238 | acc=0.4496 | macroF1=0.4373\n",
            "\n",
            "=== Evaluating model: convnext_tiny_mixed ===\n",
            "  background_floor     | n= 120 | acc=0.5167 | macroF1=0.5108\n",
            "  background_tree      | n= 120 | acc=0.2583 | macroF1=0.1887\n",
            "  background_wall      | n= 118 | acc=0.3983 | macroF1=0.4019\n",
            "  background_white     | n= 120 | acc=0.6083 | macroF1=0.5982\n",
            "  lighting_1           | n= 239 | acc=0.4561 | macroF1=0.4617\n",
            "  lighting_2           | n= 239 | acc=0.4351 | macroF1=0.4135\n",
            "  view_1               | n= 240 | acc=0.4958 | macroF1=0.4903\n",
            "  view_2               | n= 238 | acc=0.3950 | macroF1=0.3873\n",
            "\n",
            "=== Evaluating model: convnext_tiny_photo ===\n",
            "  background_floor     | n= 120 | acc=0.5167 | macroF1=0.5317\n",
            "  background_tree      | n= 120 | acc=0.3833 | macroF1=0.3303\n",
            "  background_wall      | n= 118 | acc=0.5085 | macroF1=0.4866\n",
            "  background_white     | n= 120 | acc=0.5750 | macroF1=0.5549\n",
            "  lighting_1           | n= 239 | acc=0.5314 | macroF1=0.5318\n",
            "  lighting_2           | n= 239 | acc=0.4603 | macroF1=0.4360\n",
            "  view_1               | n= 240 | acc=0.5292 | macroF1=0.5157\n",
            "  view_2               | n= 238 | acc=0.4622 | macroF1=0.4612\n",
            "\n",
            "=== Evaluating model: densenet121_baseline ===\n",
            "  background_floor     | n= 120 | acc=0.3333 | macroF1=0.3105\n",
            "  background_tree      | n= 120 | acc=0.3083 | macroF1=0.2552\n",
            "  background_wall      | n= 118 | acc=0.2712 | macroF1=0.2236\n",
            "  background_white     | n= 120 | acc=0.4750 | macroF1=0.4787\n",
            "  lighting_1           | n= 239 | acc=0.4603 | macroF1=0.4571\n",
            "  lighting_2           | n= 239 | acc=0.2343 | macroF1=0.1684\n",
            "  view_1               | n= 240 | acc=0.3917 | macroF1=0.3952\n",
            "  view_2               | n= 238 | acc=0.3025 | macroF1=0.2723\n",
            "\n",
            "=== Evaluating model: densenet121_geo ===\n",
            "  background_floor     | n= 120 | acc=0.4583 | macroF1=0.4365\n",
            "  background_tree      | n= 120 | acc=0.3417 | macroF1=0.2867\n",
            "  background_wall      | n= 118 | acc=0.3983 | macroF1=0.3821\n",
            "  background_white     | n= 120 | acc=0.6417 | macroF1=0.6367\n",
            "  lighting_1           | n= 239 | acc=0.5230 | macroF1=0.5194\n",
            "  lighting_2           | n= 239 | acc=0.3975 | macroF1=0.3709\n",
            "  view_1               | n= 240 | acc=0.4917 | macroF1=0.4795\n",
            "  view_2               | n= 238 | acc=0.4286 | macroF1=0.4149\n",
            "\n",
            "=== Evaluating model: densenet121_mix ===\n",
            "  background_floor     | n= 120 | acc=0.3750 | macroF1=0.3260\n",
            "  background_tree      | n= 120 | acc=0.3583 | macroF1=0.2857\n",
            "  background_wall      | n= 118 | acc=0.3814 | macroF1=0.3439\n",
            "  background_white     | n= 120 | acc=0.5583 | macroF1=0.5508\n",
            "  lighting_1           | n= 239 | acc=0.4268 | macroF1=0.4070\n",
            "  lighting_2           | n= 239 | acc=0.4100 | macroF1=0.3811\n",
            "  view_1               | n= 240 | acc=0.4583 | macroF1=0.4355\n",
            "  view_2               | n= 238 | acc=0.3782 | macroF1=0.3573\n",
            "\n",
            "=== Evaluating model: densenet121_photo ===\n",
            "  background_floor     | n= 120 | acc=0.5000 | macroF1=0.4887\n",
            "  background_tree      | n= 120 | acc=0.3833 | macroF1=0.3091\n",
            "  background_wall      | n= 118 | acc=0.4322 | macroF1=0.4143\n",
            "  background_white     | n= 120 | acc=0.4833 | macroF1=0.4842\n",
            "  lighting_1           | n= 239 | acc=0.5565 | macroF1=0.5554\n",
            "  lighting_2           | n= 239 | acc=0.3431 | macroF1=0.3237\n",
            "  view_1               | n= 240 | acc=0.5125 | macroF1=0.5169\n",
            "  view_2               | n= 238 | acc=0.3866 | macroF1=0.3749\n",
            "\n",
            "=== Evaluating model: resnet50_baseline ===\n",
            "  background_floor     | n= 120 | acc=0.5333 | macroF1=0.5259\n",
            "  background_tree      | n= 120 | acc=0.5917 | macroF1=0.5612\n",
            "  background_wall      | n= 118 | acc=0.4831 | macroF1=0.4404\n",
            "  background_white     | n= 120 | acc=0.6000 | macroF1=0.6016\n",
            "  lighting_1           | n= 239 | acc=0.6025 | macroF1=0.5856\n",
            "  lighting_2           | n= 239 | acc=0.5021 | macroF1=0.4748\n",
            "  view_1               | n= 240 | acc=0.5792 | macroF1=0.5656\n",
            "  view_2               | n= 238 | acc=0.5252 | macroF1=0.5069\n",
            "\n",
            "=== Evaluating model: resnet50_geo ===\n",
            "  background_floor     | n= 120 | acc=0.5500 | macroF1=0.5556\n",
            "  background_tree      | n= 120 | acc=0.5083 | macroF1=0.4814\n",
            "  background_wall      | n= 118 | acc=0.4068 | macroF1=0.3743\n",
            "  background_white     | n= 120 | acc=0.6250 | macroF1=0.6078\n",
            "  lighting_1           | n= 239 | acc=0.5649 | macroF1=0.5543\n",
            "  lighting_2           | n= 239 | acc=0.4812 | macroF1=0.4735\n",
            "  view_1               | n= 240 | acc=0.5542 | macroF1=0.5509\n",
            "  view_2               | n= 238 | acc=0.4916 | macroF1=0.4780\n",
            "\n",
            "=== Evaluating model: resnet50_mixed ===\n",
            "  background_floor     | n= 120 | acc=0.5833 | macroF1=0.5576\n",
            "  background_tree      | n= 120 | acc=0.5083 | macroF1=0.4771\n",
            "  background_wall      | n= 118 | acc=0.5169 | macroF1=0.4852\n",
            "  background_white     | n= 120 | acc=0.7083 | macroF1=0.7032\n",
            "  lighting_1           | n= 239 | acc=0.5900 | macroF1=0.5710\n",
            "  lighting_2           | n= 239 | acc=0.5690 | macroF1=0.5444\n",
            "  view_1               | n= 240 | acc=0.6292 | macroF1=0.6101\n",
            "  view_2               | n= 238 | acc=0.5294 | macroF1=0.5049\n",
            "\n",
            "=== Evaluating model: resnet50_photo ===\n",
            "  background_floor     | n= 120 | acc=0.6000 | macroF1=0.5960\n",
            "  background_tree      | n= 120 | acc=0.5833 | macroF1=0.5753\n",
            "  background_wall      | n= 118 | acc=0.5508 | macroF1=0.5364\n",
            "  background_white     | n= 120 | acc=0.6167 | macroF1=0.6224\n",
            "  lighting_1           | n= 239 | acc=0.6067 | macroF1=0.6045\n",
            "  lighting_2           | n= 239 | acc=0.5690 | macroF1=0.5757\n",
            "  view_1               | n= 240 | acc=0.6250 | macroF1=0.6282\n",
            "  view_2               | n= 238 | acc=0.5504 | macroF1=0.5531\n",
            "\n",
            "================================================================================\n",
            "Subgroup Evaluation Results for All Models\n",
            "================================================================================\n",
            "               model         subgroup  accuracy  macro_precision  macro_recall  macro_f1   n note\n",
            "  convnext_tiny_base background_floor    0.5917           0.6824        0.5917    0.5574 120     \n",
            "  convnext_tiny_base  background_tree    0.3417           0.5190        0.3417    0.2757 120     \n",
            "  convnext_tiny_base  background_wall    0.4153           0.4067        0.4083    0.3827 118     \n",
            "  convnext_tiny_base background_white    0.5750           0.7464        0.5750    0.5508 120     \n",
            "  convnext_tiny_base       lighting_1    0.5439           0.6186        0.5421    0.5244 239     \n",
            "  convnext_tiny_base       lighting_2    0.4184           0.6015        0.4169    0.3758 239     \n",
            "  convnext_tiny_base           view_1    0.5167           0.6088        0.5167    0.4901 240     \n",
            "  convnext_tiny_base           view_2    0.4454           0.6064        0.4419    0.4192 238     \n",
            "   convnext_tiny_geo background_floor    0.5917           0.7066        0.5917    0.5606 120     \n",
            "   convnext_tiny_geo  background_tree    0.3750           0.4342        0.3750    0.3559 120     \n",
            "   convnext_tiny_geo  background_wall    0.4576           0.4885        0.4509    0.4322 118     \n",
            "   convnext_tiny_geo background_white    0.5250           0.6845        0.5250    0.5212 120     \n",
            "   convnext_tiny_geo       lighting_1    0.5649           0.6194        0.5629    0.5532 239     \n",
            "   convnext_tiny_geo       lighting_2    0.4100           0.5754        0.4087    0.3774 239     \n",
            "   convnext_tiny_geo           view_1    0.5250           0.5805        0.5250    0.5024 240     \n",
            "   convnext_tiny_geo           view_2    0.4496           0.5708        0.4465    0.4373 238     \n",
            " convnext_tiny_mixed background_floor    0.5167           0.7318        0.5167    0.5108 120     \n",
            " convnext_tiny_mixed  background_tree    0.2583           0.1723        0.2583    0.1887 120     \n",
            " convnext_tiny_mixed  background_wall    0.3983           0.5035        0.3954    0.4019 118     \n",
            " convnext_tiny_mixed background_white    0.6083           0.7063        0.6083    0.5982 120     \n",
            " convnext_tiny_mixed       lighting_1    0.4561           0.5078        0.4551    0.4617 239     \n",
            " convnext_tiny_mixed       lighting_2    0.4351           0.5147        0.4337    0.4135 239     \n",
            " convnext_tiny_mixed           view_1    0.4958           0.5441        0.4958    0.4903 240     \n",
            " convnext_tiny_mixed           view_2    0.3950           0.4706        0.3923    0.3873 238     \n",
            " convnext_tiny_photo background_floor    0.5167           0.6989        0.5167    0.5317 120     \n",
            " convnext_tiny_photo  background_tree    0.3833           0.5288        0.3833    0.3303 120     \n",
            " convnext_tiny_photo  background_wall    0.5085           0.5659        0.5037    0.4866 118     \n",
            " convnext_tiny_photo background_white    0.5750           0.7361        0.5750    0.5549 120     \n",
            " convnext_tiny_photo       lighting_1    0.5314           0.6100        0.5300    0.5318 239     \n",
            " convnext_tiny_photo       lighting_2    0.4603           0.6093        0.4589    0.4360 239     \n",
            " convnext_tiny_photo           view_1    0.5292           0.6041        0.5292    0.5157 240     \n",
            " convnext_tiny_photo           view_2    0.4622           0.6004        0.4594    0.4612 238     \n",
            "densenet121_baseline background_floor    0.3333           0.5652        0.3333    0.3105 120     \n",
            "densenet121_baseline  background_tree    0.3083           0.3920        0.3083    0.2552 120     \n",
            "densenet121_baseline  background_wall    0.2712           0.5386        0.2694    0.2236 118     \n",
            "densenet121_baseline background_white    0.4750           0.6838        0.4750    0.4787 120     \n",
            "densenet121_baseline       lighting_1    0.4603           0.6646        0.4591    0.4571 239     \n",
            "densenet121_baseline       lighting_2    0.2343           0.3932        0.2341    0.1684 239     \n",
            "densenet121_baseline           view_1    0.3917           0.6755        0.3917    0.3952 240     \n",
            "densenet121_baseline           view_2    0.3025           0.5030        0.3013    0.2723 238     \n",
            "     densenet121_geo background_floor    0.4583           0.6506        0.4583    0.4365 120     \n",
            "     densenet121_geo  background_tree    0.3417           0.3501        0.3417    0.2867 120     \n",
            "     densenet121_geo  background_wall    0.3983           0.4955        0.3935    0.3821 118     \n",
            "     densenet121_geo background_white    0.6417           0.6952        0.6417    0.6367 120     \n",
            "     densenet121_geo       lighting_1    0.5230           0.5995        0.5218    0.5194 239     \n",
            "     densenet121_geo       lighting_2    0.3975           0.5431        0.3963    0.3709 239     \n",
            "     densenet121_geo           view_1    0.4917           0.5660        0.4917    0.4795 240     \n",
            "     densenet121_geo           view_2    0.4286           0.5525        0.4263    0.4149 238     \n",
            "     densenet121_mix background_floor    0.3750           0.6442        0.3750    0.3260 120     \n",
            "     densenet121_mix  background_tree    0.3583           0.3205        0.3583    0.2857 120     \n",
            "     densenet121_mix  background_wall    0.3814           0.4401        0.3750    0.3439 118     \n",
            "     densenet121_mix background_white    0.5583           0.7090        0.5583    0.5508 120     \n",
            "     densenet121_mix       lighting_1    0.4268           0.5795        0.4252    0.4070 239     \n",
            "     densenet121_mix       lighting_2    0.4100           0.5294        0.4089    0.3811 239     \n",
            "     densenet121_mix           view_1    0.4583           0.5317        0.4583    0.4355 240     \n",
            "     densenet121_mix           view_2    0.3782           0.5781        0.3757    0.3573 238     \n",
            "   densenet121_photo background_floor    0.5000           0.6089        0.5000    0.4887 120     \n",
            "   densenet121_photo  background_tree    0.3833           0.2938        0.3833    0.3091 120     \n",
            "   densenet121_photo  background_wall    0.4322           0.6195        0.4315    0.4143 118     \n",
            "   densenet121_photo background_white    0.4833           0.7205        0.4833    0.4842 120     \n",
            "   densenet121_photo       lighting_1    0.5565           0.6155        0.5556    0.5554 239     \n",
            "   densenet121_photo       lighting_2    0.3431           0.6171        0.3428    0.3237 239     \n",
            "   densenet121_photo           view_1    0.5125           0.6462        0.5125    0.5169 240     \n",
            "   densenet121_photo           view_2    0.3866           0.5376        0.3849    0.3749 238     \n",
            "   resnet50_baseline background_floor    0.5333           0.7282        0.5333    0.5259 120     \n",
            "   resnet50_baseline  background_tree    0.5917           0.6658        0.5917    0.5612 120     \n",
            "   resnet50_baseline  background_wall    0.4831           0.5759        0.4769    0.4404 118     \n",
            "   resnet50_baseline background_white    0.6000           0.7375        0.6000    0.6016 120     \n",
            "   resnet50_baseline       lighting_1    0.6025           0.6800        0.6010    0.5856 239     \n",
            "   resnet50_baseline       lighting_2    0.5021           0.6157        0.5005    0.4748 239     \n",
            "   resnet50_baseline           view_1    0.5792           0.6799        0.5792    0.5656 240     \n",
            "   resnet50_baseline           view_2    0.5252           0.5986        0.5219    0.5069 238     \n",
            "        resnet50_geo background_floor    0.5500           0.6596        0.5500    0.5556 120     \n",
            "        resnet50_geo  background_tree    0.5083           0.5799        0.5083    0.4814 120     \n",
            "        resnet50_geo  background_wall    0.4068           0.5414        0.4028    0.3743 118     \n",
            "        resnet50_geo background_white    0.6250           0.6847        0.6250    0.6078 120     \n",
            "        resnet50_geo       lighting_1    0.5649           0.6233        0.5631    0.5543 239     \n",
            "        resnet50_geo       lighting_2    0.4812           0.5918        0.4800    0.4735 239     \n",
            "        resnet50_geo           view_1    0.5542           0.6222        0.5542    0.5509 240     \n",
            "        resnet50_geo           view_2    0.4916           0.5952        0.4884    0.4780 238     \n",
            "      resnet50_mixed background_floor    0.5833           0.7015        0.5833    0.5576 120     \n",
            "      resnet50_mixed  background_tree    0.5083           0.4963        0.5083    0.4771 120     \n",
            "      resnet50_mixed  background_wall    0.5169           0.5868        0.5093    0.4852 118     \n",
            "      resnet50_mixed background_white    0.7083           0.8073        0.7083    0.7032 120     \n",
            "      resnet50_mixed       lighting_1    0.5900           0.6701        0.5880    0.5710 239     \n",
            "      resnet50_mixed       lighting_2    0.5690           0.6719        0.5672    0.5444 239     \n",
            "      resnet50_mixed           view_1    0.6292           0.6942        0.6292    0.6101 240     \n",
            "      resnet50_mixed           view_2    0.5294           0.6590        0.5254    0.5049 238     \n",
            "      resnet50_photo background_floor    0.6000           0.6746        0.6000    0.5960 120     \n",
            "      resnet50_photo  background_tree    0.5833           0.6279        0.5833    0.5753 120     \n",
            "      resnet50_photo  background_wall    0.5508           0.6861        0.5463    0.5364 118     \n",
            "      resnet50_photo background_white    0.6167           0.7527        0.6167    0.6224 120     \n",
            "      resnet50_photo       lighting_1    0.6067           0.6612        0.6056    0.6045 239     \n",
            "      resnet50_photo       lighting_2    0.5690           0.6717        0.5682    0.5757 239     \n",
            "      resnet50_photo           view_1    0.6250           0.6972        0.6250    0.6282 240     \n",
            "      resnet50_photo           view_2    0.5504           0.6279        0.5485    0.5531 238     \n"
          ]
        }
      ],
      "source": [
        "# Evaluate all models on all subgroups\n",
        "subgroups_root = Path(\"subgroups\")  \n",
        "cnn_models_dir_path = Path(cnn_models_dir) \n",
        "rows = []\n",
        "\n",
        "for entry in sorted(cnn_models_dir_path.iterdir()):\n",
        "    if not entry.name.endswith(\".pth\"):\n",
        "        continue\n",
        "\n",
        "    model_name = entry.stem\n",
        "    print(f\"\\n=== Evaluating model: {model_name} ===\")\n",
        "\n",
        "    model = rebuild_model(entry.name, num_classes, device)\n",
        "    ckpt = torch.load(entry, map_location=device)\n",
        "    model.load_state_dict(ckpt[\"model_state_dict\"])\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    for sg_dir in sorted(subgroups_root.iterdir()):\n",
        "        if not sg_dir.is_dir():\n",
        "            continue\n",
        "\n",
        "        ds = datasets.ImageFolder(sg_dir, transform=eval_tf)\n",
        "\n",
        "        if len(ds) == 0:\n",
        "            rows.append({\n",
        "                \"model\": model_name,\n",
        "                \"subgroup\": sg_dir.name,\n",
        "                \"accuracy\": float(\"nan\"),\n",
        "                \"macro_precision\": float(\"nan\"),\n",
        "                \"macro_recall\": float(\"nan\"),\n",
        "                \"macro_f1\": float(\"nan\"),\n",
        "                \"n\": 0,\n",
        "                \"note\": \"empty subgroup\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            ds,\n",
        "            batch_size=32,\n",
        "            shuffle=False,\n",
        "            num_workers=0 \n",
        "        )\n",
        "\n",
        "        metrics = evaluate_model_metrics(model, loader, device)\n",
        "\n",
        "        rows.append({\n",
        "            \"model\": model_name,\n",
        "            \"subgroup\": sg_dir.name,\n",
        "            \"accuracy\": metrics[\"accuracy\"],\n",
        "            \"macro_precision\": metrics[\"macro_precision\"],\n",
        "            \"macro_recall\": metrics[\"macro_recall\"],\n",
        "            \"macro_f1\": metrics[\"macro_f1\"],\n",
        "            \"n\": metrics[\"n\"],\n",
        "            \"note\": \"\"\n",
        "        })\n",
        "\n",
        "        print(\n",
        "            f\"  {sg_dir.name:20s} | \"\n",
        "            f\"n={metrics['n']:4d} | \"\n",
        "            f\"acc={metrics['accuracy']:.4f} | \"\n",
        "            f\"macroF1={metrics['macro_f1']:.4f}\"\n",
        "        )\n",
        "\n",
        "    del model\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "# Create DataFrame and display results\n",
        "df_subgroups = pd.DataFrame(rows)\n",
        "metric_cols = [c for c in df_subgroups.columns if c not in [\"model\", \"subgroup\", \"n\", \"note\"]]\n",
        "df_subgroups[metric_cols] = df_subgroups[metric_cols].round(4)\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"Subgroup Evaluation Results for All Models\")\n",
        "print(\"=\"*80)\n",
        "print(df_subgroups.to_string(index=False))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (thesis venv)",
      "language": "python",
      "name": "thesis-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
