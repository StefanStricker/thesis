{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "22d20b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "import time\n",
        "import copy\n",
        "import shutil"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "d0ba08e5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.9.0+cu128'"
            ]
          },
          "execution_count": 35,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "b7612515",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_path = Path(\"data/datasets/trashnet_01/\")\n",
        "input_path2 = Path(\"data/datasets/self-collected_outdoor/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "e1d693c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed = 16\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "e4dbb0bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6821f7d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
            "Train counts: Counter({3: 475, 1: 400, 4: 385, 2: 328, 0: 322, 5: 109})\n",
            "Class weights: tensor([0.8270, 0.6657, 0.8119, 0.5606, 0.6917, 2.4431], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_class_weights_from_imagefolder(train_root, num_classes):\n",
        "    ds = datasets.ImageFolder(train_root, transform=transforms.ToTensor())\n",
        "    counts = Counter(ds.targets)\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    weights = [total / counts[i] for i in range(num_classes)]\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    weights = weights / weights.sum() * num_classes \n",
        "    return weights, ds.classes, counts\n",
        "\n",
        "tmp_ds = datasets.ImageFolder(input_path / \"train\")\n",
        "num_classes = len(tmp_ds.classes)\n",
        "\n",
        "class_weights, class_names, train_counts = compute_class_weights_from_imagefolder(\n",
        "    input_path / \"train\",\n",
        "    num_classes\n",
        ")\n",
        "\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "criterion_weighted = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Train counts:\", train_counts)\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "55b330fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# baseline data transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),  \n",
        "        transforms.RandomHorizontalFlip(), \n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),    \n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms[\"val\"]),\n",
        "\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms[\"val\"])\n",
        "}\n",
        "\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}   \n",
        "\n",
        "\n",
        "dataset_size = {\n",
        "    \"train\": len(image_datasets[\"train\"]),\n",
        "    \"val\": len(image_datasets[\"val\"]),\n",
        "    \"test_01\": len(image_datasets[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes = len((image_datasets[\"train\"]).classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "50c0f428",
      "metadata": {},
      "outputs": [],
      "source": [
        "#geometric augmentation\n",
        "\n",
        "data_transforms_geo = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_geo = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_geo[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_geo[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_geo[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_geo[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_geo = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_geo[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_geo[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_geo[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_geo[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_geo = {\n",
        "    \"train\": len(image_datasets_geo[\"train\"]),\n",
        "    \"val\": len(image_datasets_geo[\"val\"]),\n",
        "    \"test\": len(image_datasets_geo[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_geo[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_geo = len(image_datasets_geo[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "c86eda2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#photometric\n",
        " \n",
        "data_transforms_photo = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_photo = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_photo[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_photo[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_photo[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_photo[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_photo = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_photo[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_photo[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_photo[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_photo[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_photo = {\n",
        "    \"train\": len(image_datasets_photo[\"train\"]),\n",
        "    \"val\": len(image_datasets_photo[\"val\"]),\n",
        "    \"test\": len(image_datasets_photo[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_photo[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_photo = len(image_datasets_photo[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "d1a6de36",
      "metadata": {},
      "outputs": [],
      "source": [
        "#aug mix\n",
        " \n",
        "data_transforms_mix = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),\n",
        "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random')\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_mix = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_mix[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_mix[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_mix[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_mix[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_mix = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_mix[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_mix[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_mix[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_mix[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_mix = {\n",
        "    \"train\": len(image_datasets_mix[\"train\"]),\n",
        "    \"val\": len(image_datasets_mix[\"val\"]),\n",
        "    \"test\": len(image_datasets_mix[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_mix[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_mix = len(image_datasets_mix[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "abaa0b3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#MixUp function\n",
        "\n",
        "def apply_mixup(inputs, labels, alpha=0.4):\n",
        "    if alpha <= 0:\n",
        "        return inputs, labels, labels, 1.0\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = inputs.size(0)\n",
        "    index = torch.randperm(batch_size, device=inputs.device)\n",
        "\n",
        "    mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
        "    labels_a = labels\n",
        "    labels_b = labels[index]\n",
        "    return mixed_inputs, labels_a, labels_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "b14f42c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, dataset_size, device, criterion, optimizer, num_epochs = 25, use_mixup=False, mixup_prob=0.5, mixup_alpha=0.4):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "\n",
        "                        if phase == \"train\" and use_mixup and (np.random.rand() < mixup_prob):\n",
        "                            inputs_mixed, y_a, y_b, lam = apply_mixup(inputs, labels, alpha=mixup_alpha)\n",
        "\n",
        "                            outputs = model(inputs_mixed)\n",
        "                            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "\n",
        "                            running_corrects += (float(lam) * torch.sum(preds == y_a).item() +\n",
        "                                                (1 - float(lam)) * torch.sum(preds == y_b).item())\n",
        "                        else:\n",
        "                            outputs = model(inputs)\n",
        "                            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                            loss = criterion(outputs, labels)\n",
        "                            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "                        if phase == \"train\":\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                        \n",
        "                running_loss += loss.item() * inputs.size(0)      \n",
        "\n",
        "            epoch_loss = running_loss / dataset_size[phase]\n",
        "            epoch_acc = running_corrects / dataset_size[phase]\n",
        "\n",
        "            print(f\"{phase} Loss :{epoch_loss:.4f} Acc: {epoch_acc:.4f}\")     \n",
        "\n",
        "            if phase == \"train\":\n",
        "                history[\"train_loss\"].append(epoch_loss)    \n",
        "                history[\"train_acc\"].append(epoch_acc)   \n",
        "            else:\n",
        "                history[\"val_loss\"].append(epoch_loss)  \n",
        "                history[\"val_acc\"].append(epoch_acc)  \n",
        "\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()              \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "51cb155f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model_metrics(model, dataloader, device):\n",
        "    model.eval()\n",
        "    preds_list, labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            preds_list.extend(preds.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels_list, preds_list)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_list, preds_list, average=\"macro\", zero_division=0\n",
        "    )\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": acc,\n",
        "        \"macro_precision\": precision,\n",
        "        \"macro_recall\": recall,\n",
        "        \"macro_f1\": f1,\n",
        "        \"n\": len(labels_list),\n",
        "    }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "9f005bcc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def build_subgroup_folder(\n",
        "    csv_path,\n",
        "    images_root,\n",
        "    out_root,\n",
        "    filter_col,\n",
        "    filter_value,        \n",
        "    class_col=\"class\",\n",
        "    id_col=\"id\",\n",
        "    overwrite=True,\n",
        "    copy_files=True    \n",
        "):\n",
        "    csv_path = Path(csv_path)\n",
        "    images_root = Path(images_root)\n",
        "    out_root = Path(out_root)\n",
        "\n",
        "    df = pd.read_csv(csv_path)\n",
        "\n",
        "    sub = df[df[filter_col] == filter_value].copy()\n",
        "    subgroup_name = f\"{filter_col}_{filter_value}\"\n",
        "    dst_root = out_root / subgroup_name\n",
        "\n",
        "    if overwrite and dst_root.exists():\n",
        "        shutil.rmtree(dst_root)\n",
        "    dst_root.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    missing = 0\n",
        "    written = 0\n",
        "\n",
        "    for _, row in sub.iterrows():\n",
        "        cls = str(row[class_col]).strip()\n",
        "        fname = str(row[id_col]).strip()\n",
        "\n",
        "        src = images_root / cls / fname\n",
        "        if not src.exists():\n",
        "            missing += 1\n",
        "            continue\n",
        "\n",
        "        dst_class_dir = dst_root / cls\n",
        "        dst_class_dir.mkdir(parents=True, exist_ok=True)\n",
        "        dst = dst_class_dir / fname\n",
        "\n",
        "        if copy_files:\n",
        "            shutil.copy2(src, dst)\n",
        "        else:\n",
        "            if dst.exists():\n",
        "                dst.unlink()\n",
        "            dst.symlink_to(src.resolve())\n",
        "\n",
        "        written += 1\n",
        "        \n",
        "    return dst_root\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "829db3aa",
      "metadata": {},
      "outputs": [],
      "source": [
        "csv_path = \"subgroups.csv\"\n",
        "images_root = input_path2\n",
        "out_root = \"subgroups\"\n",
        "\n",
        "low_light_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"lighting\", filter_value=2,\n",
        "                                       copy_files=True)\n",
        "\n",
        "good_light_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                        filter_col=\"lighting\", filter_value=1,\n",
        "                                        copy_files=True)\n",
        "\n",
        "background_tree_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"tree\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_wall_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"wall\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_white_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"white\",\n",
        "                                       copy_files=True)\n",
        "\n",
        "background_floor_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"background\", filter_value=\"floor\",\n",
        "                                       copy_files=True)  \n",
        "\n",
        "near_view_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                       filter_col=\"view\", filter_value=1,\n",
        "                                       copy_files=True)\n",
        "\n",
        "far_view_root = build_subgroup_folder(csv_path, images_root, out_root,\n",
        "                                        filter_col=\"view\", filter_value=2,\n",
        "                                        copy_files=True)                                                                                                                   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "16d676a2",
      "metadata": {},
      "outputs": [],
      "source": [
        "def evaluate_all_subgroups(\n",
        "    model,\n",
        "    subgroups_root,       \n",
        "    transform,            \n",
        "    device,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    print_results=True\n",
        "):\n",
        "\n",
        "    subgroups_root = Path(subgroups_root)\n",
        "    assert subgroups_root.exists(), f\"Subgroups root not found: {subgroups_root}\"\n",
        "\n",
        "    results = []\n",
        "    subgroup_dirs = sorted([p for p in subgroups_root.iterdir() if p.is_dir()])\n",
        "\n",
        "    for sg_dir in subgroup_dirs:\n",
        "        has_class_dirs = any([p.is_dir() for p in sg_dir.iterdir()])\n",
        "        if not has_class_dirs:\n",
        "            results.append({\n",
        "                \"subgroup\": sg_dir.name,\n",
        "                \"accuracy\": np.nan,\n",
        "                \"macro_precision\": np.nan,\n",
        "                \"macro_recall\": np.nan,\n",
        "                \"macro_f1\": np.nan,\n",
        "                \"n\": 0,\n",
        "                \"note\": \"empty subgroup folder\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        ds = datasets.ImageFolder(sg_dir, transform=transform)\n",
        "\n",
        "        if len(ds) == 0:\n",
        "            results.append({\n",
        "                \"subgroup\": sg_dir.name,\n",
        "                \"accuracy\": np.nan,\n",
        "                \"macro_precision\": np.nan,\n",
        "                \"macro_recall\": np.nan,\n",
        "                \"macro_f1\": np.nan,\n",
        "                \"n\": 0,\n",
        "                \"note\": \"no images\"\n",
        "            })\n",
        "            continue\n",
        "\n",
        "        loader = torch.utils.data.DataLoader(\n",
        "            ds, batch_size=batch_size, shuffle=False, num_workers=num_workers\n",
        "        )\n",
        "\n",
        "        metrics = evaluate_model_metrics(model, loader, device)\n",
        "        metrics[\"subgroup\"] = sg_dir.name\n",
        "        metrics[\"note\"] = \"\"\n",
        "        results.append(metrics)\n",
        "\n",
        "        if print_results:\n",
        "            print(f\"{sg_dir.name:20s} | n={metrics['n']:4d} | \"\n",
        "                  f\"acc={metrics['accuracy']:.4f} | macroF1={metrics['macro_f1']:.4f}\")\n",
        "\n",
        "    df = pd.DataFrame(results).sort_values(by=\"subgroup\").reset_index(drop=True)\n",
        "    return df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "74293688",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7872 Acc: 0.1887\n",
            "val Loss :1.7691 Acc: 0.2032\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7384 Acc: 0.2947\n",
            "val Loss :1.7137 Acc: 0.3745\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.6961 Acc: 0.4121\n",
            "val Loss :1.6596 Acc: 0.5060\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6556 Acc: 0.4953\n",
            "val Loss :1.6109 Acc: 0.5976\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6140 Acc: 0.5527\n",
            "val Loss :1.5797 Acc: 0.6653\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.5681 Acc: 0.5973\n",
            "val Loss :1.5045 Acc: 0.7211\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5154 Acc: 0.6038\n",
            "val Loss :1.4555 Acc: 0.7450\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.4664 Acc: 0.6236\n",
            "val Loss :1.3833 Acc: 0.7610\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.4233 Acc: 0.6444\n",
            "val Loss :1.3649 Acc: 0.7291\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.3714 Acc: 0.6568\n",
            "val Loss :1.2840 Acc: 0.7450\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.3146 Acc: 0.6642\n",
            "val Loss :1.2224 Acc: 0.7649\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.2737 Acc: 0.6657\n",
            "val Loss :1.1630 Acc: 0.7410\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.2215 Acc: 0.6880\n",
            "val Loss :1.0990 Acc: 0.7689\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.1749 Acc: 0.6875\n",
            "val Loss :1.0667 Acc: 0.7729\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.1368 Acc: 0.6919\n",
            "val Loss :1.0140 Acc: 0.7888\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.0969 Acc: 0.6994\n",
            "val Loss :0.9544 Acc: 0.7968\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.0540 Acc: 0.7152\n",
            "val Loss :0.9403 Acc: 0.7888\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.0268 Acc: 0.7103\n",
            "val Loss :0.8622 Acc: 0.8088\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.0033 Acc: 0.7127\n",
            "val Loss :0.8294 Acc: 0.7928\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :0.9634 Acc: 0.7112\n",
            "val Loss :0.8033 Acc: 0.8008\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :0.9358 Acc: 0.7286\n",
            "val Loss :0.8123 Acc: 0.8127\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.9029 Acc: 0.7434\n",
            "val Loss :0.7184 Acc: 0.8167\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.8750 Acc: 0.7345\n",
            "val Loss :0.7108 Acc: 0.8247\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.8472 Acc: 0.7449\n",
            "val Loss :0.6868 Acc: 0.8167\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.8159 Acc: 0.7484\n",
            "val Loss :0.6828 Acc: 0.8207\n",
            "\n",
            "Training complete in 4m 7s\n",
            "Best val Acc: 0.8247\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50\n",
        "\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet, resnet_history = train_model(\n",
        "    model=resnet,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "fbace900",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net ID metrics\n",
            "{'accuracy': 0.8054474708171206, 'macro_precision': 0.7726845522898155, 'macro_recall': 0.7791504406640705, 'macro_f1': 0.7713021970082439, 'n': 257}\n",
            "Res-Net OOD metrics\n",
            "{'accuracy': 0.5627615062761506, 'macro_precision': 0.5843147165796183, 'macro_recall': 0.5625, 'macro_f1': 0.5630956405516339, 'n': 478}\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net ID metrics\")\n",
        "test_metrics = evaluate_model_metrics(resnet, dataloaders_photo[\"test\"], device)\n",
        "print(test_metrics)\n",
        "print(\"Res-Net OOD metrics\")\n",
        "ood_metrics = evaluate_model_metrics(resnet, dataloaders_photo[\"test_01\"], device)\n",
        "print(ood_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "8154c2cf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "background_floor     | n= 120 | acc=0.6083 | macroF1=0.5758\n",
            "background_tree      | n= 120 | acc=0.4917 | macroF1=0.4808\n",
            "background_wall      | n= 118 | acc=0.6356 | macroF1=0.6296\n",
            "background_white     | n= 120 | acc=0.5167 | macroF1=0.5288\n",
            "lighting_1           | n= 239 | acc=0.6067 | macroF1=0.6050\n",
            "lighting_2           | n= 239 | acc=0.5188 | macroF1=0.5165\n",
            "view_1               | n= 240 | acc=0.5625 | macroF1=0.5567\n",
            "view_2               | n= 238 | acc=0.5630 | macroF1=0.5667\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_precision</th>\n",
              "      <th>macro_recall</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>n</th>\n",
              "      <th>subgroup</th>\n",
              "      <th>note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.608333</td>\n",
              "      <td>0.595732</td>\n",
              "      <td>0.608333</td>\n",
              "      <td>0.575806</td>\n",
              "      <td>120</td>\n",
              "      <td>background_floor</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.491667</td>\n",
              "      <td>0.571825</td>\n",
              "      <td>0.491667</td>\n",
              "      <td>0.480798</td>\n",
              "      <td>120</td>\n",
              "      <td>background_tree</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.635593</td>\n",
              "      <td>0.674481</td>\n",
              "      <td>0.636111</td>\n",
              "      <td>0.629641</td>\n",
              "      <td>118</td>\n",
              "      <td>background_wall</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.573967</td>\n",
              "      <td>0.516667</td>\n",
              "      <td>0.528848</td>\n",
              "      <td>120</td>\n",
              "      <td>background_white</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.606695</td>\n",
              "      <td>0.616825</td>\n",
              "      <td>0.606303</td>\n",
              "      <td>0.605011</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.518828</td>\n",
              "      <td>0.566485</td>\n",
              "      <td>0.518697</td>\n",
              "      <td>0.516467</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.584694</td>\n",
              "      <td>0.562500</td>\n",
              "      <td>0.556713</td>\n",
              "      <td>240</td>\n",
              "      <td>view_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.563025</td>\n",
              "      <td>0.584650</td>\n",
              "      <td>0.562939</td>\n",
              "      <td>0.566749</td>\n",
              "      <td>238</td>\n",
              "      <td>view_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  macro_precision  macro_recall  macro_f1    n          subgroup  \\\n",
              "0  0.608333         0.595732      0.608333  0.575806  120  background_floor   \n",
              "1  0.491667         0.571825      0.491667  0.480798  120   background_tree   \n",
              "2  0.635593         0.674481      0.636111  0.629641  118   background_wall   \n",
              "3  0.516667         0.573967      0.516667  0.528848  120  background_white   \n",
              "4  0.606695         0.616825      0.606303  0.605011  239        lighting_1   \n",
              "5  0.518828         0.566485      0.518697  0.516467  239        lighting_2   \n",
              "6  0.562500         0.584694      0.562500  0.556713  240            view_1   \n",
              "7  0.563025         0.584650      0.562939  0.566749  238            view_2   \n",
              "\n",
              "  note  \n",
              "0       \n",
              "1       \n",
              "2       \n",
              "3       \n",
              "4       \n",
              "5       \n",
              "6       \n",
              "7       "
            ]
          },
          "execution_count": 51,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_eval = data_transforms[\"val\"]\n",
        "\n",
        "df_subgroups = evaluate_all_subgroups(\n",
        "    model=resnet,      \n",
        "    subgroups_root=\"subgroups\",\n",
        "    transform=transform_eval,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    print_results=True\n",
        ")\n",
        "\n",
        "df_subgroups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "e938db82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7935 Acc: 0.1872\n",
            "val Loss :1.7676 Acc: 0.2151\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7519 Acc: 0.2754\n",
            "val Loss :1.7240 Acc: 0.3227\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7170 Acc: 0.3596\n",
            "val Loss :1.6705 Acc: 0.4462\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6781 Acc: 0.4235\n",
            "val Loss :1.6297 Acc: 0.5498\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6384 Acc: 0.5007\n",
            "val Loss :1.5802 Acc: 0.5737\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.5920 Acc: 0.5617\n",
            "val Loss :1.5305 Acc: 0.6175\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5565 Acc: 0.5914\n",
            "val Loss :1.4906 Acc: 0.6414\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.5089 Acc: 0.5835\n",
            "val Loss :1.4255 Acc: 0.6454\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.4618 Acc: 0.6107\n",
            "val Loss :1.3714 Acc: 0.6813\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.4204 Acc: 0.6424\n",
            "val Loss :1.3602 Acc: 0.6853\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.3934 Acc: 0.6191\n",
            "val Loss :1.2643 Acc: 0.7012\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.3401 Acc: 0.6365\n",
            "val Loss :1.2403 Acc: 0.7131\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.2945 Acc: 0.6498\n",
            "val Loss :1.1739 Acc: 0.7171\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.2542 Acc: 0.6538\n",
            "val Loss :1.1294 Acc: 0.7371\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.2185 Acc: 0.6454\n",
            "val Loss :1.0780 Acc: 0.7291\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.1814 Acc: 0.6682\n",
            "val Loss :1.0355 Acc: 0.7689\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.1418 Acc: 0.6662\n",
            "val Loss :1.0037 Acc: 0.7610\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.1102 Acc: 0.6731\n",
            "val Loss :0.9545 Acc: 0.7649\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.0808 Acc: 0.6766\n",
            "val Loss :0.9141 Acc: 0.7410\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.0305 Acc: 0.6984\n",
            "val Loss :0.9221 Acc: 0.7888\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.0123 Acc: 0.6890\n",
            "val Loss :0.8749 Acc: 0.7888\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.9874 Acc: 0.6924\n",
            "val Loss :0.8085 Acc: 0.7888\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.9558 Acc: 0.7028\n",
            "val Loss :0.7711 Acc: 0.7928\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.9492 Acc: 0.7023\n",
            "val Loss :0.7960 Acc: 0.8127\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.9187 Acc: 0.7048\n",
            "val Loss :0.7180 Acc: 0.8048\n",
            "\n",
            "Training complete in 4m 21s\n",
            "Best val Acc: 0.8127\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 photo augmentations\n",
        "\n",
        "resnet_aug_1 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_aug_1.fc = nn.Linear(resnet_aug_1.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_aug_1 = resnet_aug_1.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_aug_1.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_aug_1, resnet_history_aug_1 = train_model(\n",
        "    model=resnet_aug_1,\n",
        "    dataloaders=dataloaders_photo,\n",
        "    dataset_size=dataset_size_photo,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "34c036c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net ID metrics\n",
            "{'accuracy': 0.7315175097276264, 'macro_precision': 0.69988556236434, 'macro_recall': 0.7136195941791351, 'macro_f1': 0.6997284882001963, 'n': 257}\n",
            "Res-Net OOD metrics\n",
            "{'accuracy': 0.5878661087866108, 'macro_precision': 0.6325503186124876, 'macro_recall': 0.5879807692307693, 'macro_f1': 0.5774128070370734, 'n': 478}\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net ID metrics\")\n",
        "test_metrics = evaluate_model_metrics(resnet_aug_1, dataloaders_photo[\"test\"], device)\n",
        "print(test_metrics)\n",
        "print(\"Res-Net OOD metrics\")\n",
        "ood_metrics = evaluate_model_metrics(resnet_aug_1, dataloaders_photo[\"test_01\"], device)\n",
        "print(ood_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "40a9a81a",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "background_floor     | n= 120 | acc=0.6417 | macroF1=0.6068\n",
            "background_tree      | n= 120 | acc=0.5917 | macroF1=0.5807\n",
            "background_wall      | n= 118 | acc=0.5763 | macroF1=0.5583\n",
            "background_white     | n= 120 | acc=0.5417 | macroF1=0.5363\n",
            "lighting_1           | n= 239 | acc=0.5941 | macroF1=0.5780\n",
            "lighting_2           | n= 239 | acc=0.5816 | macroF1=0.5752\n",
            "view_1               | n= 240 | acc=0.6208 | macroF1=0.6137\n",
            "view_2               | n= 238 | acc=0.5546 | macroF1=0.5390\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_precision</th>\n",
              "      <th>macro_recall</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>n</th>\n",
              "      <th>subgroup</th>\n",
              "      <th>note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.633554</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>120</td>\n",
              "      <td>background_floor</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.665936</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.580720</td>\n",
              "      <td>120</td>\n",
              "      <td>background_tree</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.576271</td>\n",
              "      <td>0.693355</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.558293</td>\n",
              "      <td>118</td>\n",
              "      <td>background_wall</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.590424</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.536331</td>\n",
              "      <td>120</td>\n",
              "      <td>background_white</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.594142</td>\n",
              "      <td>0.646218</td>\n",
              "      <td>0.594124</td>\n",
              "      <td>0.578012</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.581590</td>\n",
              "      <td>0.625870</td>\n",
              "      <td>0.581838</td>\n",
              "      <td>0.575192</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.620833</td>\n",
              "      <td>0.668802</td>\n",
              "      <td>0.620833</td>\n",
              "      <td>0.613702</td>\n",
              "      <td>240</td>\n",
              "      <td>view_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.554622</td>\n",
              "      <td>0.602522</td>\n",
              "      <td>0.554605</td>\n",
              "      <td>0.538991</td>\n",
              "      <td>238</td>\n",
              "      <td>view_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  macro_precision  macro_recall  macro_f1    n          subgroup  \\\n",
              "0  0.641667         0.633554      0.641667  0.606805  120  background_floor   \n",
              "1  0.591667         0.665936      0.591667  0.580720  120   background_tree   \n",
              "2  0.576271         0.693355      0.577778  0.558293  118   background_wall   \n",
              "3  0.541667         0.590424      0.541667  0.536331  120  background_white   \n",
              "4  0.594142         0.646218      0.594124  0.578012  239        lighting_1   \n",
              "5  0.581590         0.625870      0.581838  0.575192  239        lighting_2   \n",
              "6  0.620833         0.668802      0.620833  0.613702  240            view_1   \n",
              "7  0.554622         0.602522      0.554605  0.538991  238            view_2   \n",
              "\n",
              "  note  \n",
              "0       \n",
              "1       \n",
              "2       \n",
              "3       \n",
              "4       \n",
              "5       \n",
              "6       \n",
              "7       "
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_eval = data_transforms[\"val\"]\n",
        "\n",
        "df_subgroups = evaluate_all_subgroups(\n",
        "    model=resnet_aug_1,      \n",
        "    subgroups_root=\"subgroups\",\n",
        "    transform=transform_eval,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    print_results=True\n",
        ")\n",
        "\n",
        "df_subgroups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "416b1b32",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7795 Acc: 0.1823\n",
            "val Loss :1.7565 Acc: 0.2629\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7456 Acc: 0.2858\n",
            "val Loss :1.7143 Acc: 0.3625\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7095 Acc: 0.3947\n",
            "val Loss :1.6791 Acc: 0.4661\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6837 Acc: 0.4443\n",
            "val Loss :1.6317 Acc: 0.5418\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6377 Acc: 0.5102\n",
            "val Loss :1.5825 Acc: 0.6056\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.6064 Acc: 0.5433\n",
            "val Loss :1.5409 Acc: 0.6494\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5676 Acc: 0.5765\n",
            "val Loss :1.4944 Acc: 0.6653\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.5235 Acc: 0.5810\n",
            "val Loss :1.4791 Acc: 0.6693\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.4764 Acc: 0.6033\n",
            "val Loss :1.4226 Acc: 0.6972\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.4316 Acc: 0.6221\n",
            "val Loss :1.3447 Acc: 0.7012\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.3850 Acc: 0.6275\n",
            "val Loss :1.3038 Acc: 0.7092\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.3361 Acc: 0.6563\n",
            "val Loss :1.2686 Acc: 0.7410\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.2980 Acc: 0.6533\n",
            "val Loss :1.2023 Acc: 0.7371\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.2596 Acc: 0.6474\n",
            "val Loss :1.1597 Acc: 0.7490\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.2209 Acc: 0.6627\n",
            "val Loss :1.1271 Acc: 0.7610\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.1798 Acc: 0.6677\n",
            "val Loss :1.0677 Acc: 0.7649\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.1529 Acc: 0.6701\n",
            "val Loss :1.0331 Acc: 0.7809\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.1057 Acc: 0.6706\n",
            "val Loss :0.9903 Acc: 0.7689\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.0715 Acc: 0.6904\n",
            "val Loss :0.9455 Acc: 0.7729\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.0416 Acc: 0.6934\n",
            "val Loss :0.9162 Acc: 0.7570\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.0247 Acc: 0.7033\n",
            "val Loss :0.8825 Acc: 0.7968\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.9908 Acc: 0.7103\n",
            "val Loss :0.8500 Acc: 0.7968\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.9610 Acc: 0.7053\n",
            "val Loss :0.8302 Acc: 0.7968\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.9243 Acc: 0.7256\n",
            "val Loss :0.7892 Acc: 0.8008\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.9080 Acc: 0.7147\n",
            "val Loss :0.7794 Acc: 0.8008\n",
            "\n",
            "Training complete in 4m 6s\n",
            "Best val Acc: 0.8008\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 geo augmentations\n",
        "\n",
        "resnet_aug_2 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_aug_2.fc = nn.Linear(resnet_aug_2.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_aug_2 = resnet_aug_2.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_aug_2.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_aug_2, resnet_history_aug_2 = train_model(\n",
        "    model=resnet_aug_2,\n",
        "    dataloaders=dataloaders_geo,\n",
        "    dataset_size=dataset_size_geo,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "cb2bd59f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net Geo ID metrics\n",
            "{'accuracy': 0.7587548638132295, 'macro_precision': 0.7434927800749688, 'macro_recall': 0.7253181760509082, 'macro_f1': 0.7297055692898495, 'n': 257}\n",
            "Res-Net Geo OOD metrics\n",
            "{'accuracy': 0.50418410041841, 'macro_precision': 0.5652927997355243, 'macro_recall': 0.5028311965811966, 'macro_f1': 0.4885215034156976, 'n': 478}\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net Geo ID metrics\")\n",
        "test_metrics = evaluate_model_metrics(resnet_aug_2, dataloaders_geo[\"test\"], device)\n",
        "print(test_metrics)\n",
        "print(\"Res-Net Geo OOD metrics\")\n",
        "ood_metrics = evaluate_model_metrics(resnet_aug_2, dataloaders_geo[\"test_01\"], device)\n",
        "print(ood_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "247e5781",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "background_floor     | n= 120 | acc=0.6417 | macroF1=0.6068\n",
            "background_tree      | n= 120 | acc=0.5917 | macroF1=0.5807\n",
            "background_wall      | n= 118 | acc=0.5763 | macroF1=0.5583\n",
            "background_white     | n= 120 | acc=0.5417 | macroF1=0.5363\n",
            "lighting_1           | n= 239 | acc=0.5941 | macroF1=0.5780\n",
            "lighting_2           | n= 239 | acc=0.5816 | macroF1=0.5752\n",
            "view_1               | n= 240 | acc=0.6208 | macroF1=0.6137\n",
            "view_2               | n= 238 | acc=0.5546 | macroF1=0.5390\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_precision</th>\n",
              "      <th>macro_recall</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>n</th>\n",
              "      <th>subgroup</th>\n",
              "      <th>note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.633554</td>\n",
              "      <td>0.641667</td>\n",
              "      <td>0.606805</td>\n",
              "      <td>120</td>\n",
              "      <td>background_floor</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.665936</td>\n",
              "      <td>0.591667</td>\n",
              "      <td>0.580720</td>\n",
              "      <td>120</td>\n",
              "      <td>background_tree</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.576271</td>\n",
              "      <td>0.693355</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.558293</td>\n",
              "      <td>118</td>\n",
              "      <td>background_wall</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.590424</td>\n",
              "      <td>0.541667</td>\n",
              "      <td>0.536331</td>\n",
              "      <td>120</td>\n",
              "      <td>background_white</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.594142</td>\n",
              "      <td>0.646218</td>\n",
              "      <td>0.594124</td>\n",
              "      <td>0.578012</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.581590</td>\n",
              "      <td>0.625870</td>\n",
              "      <td>0.581838</td>\n",
              "      <td>0.575192</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.620833</td>\n",
              "      <td>0.668802</td>\n",
              "      <td>0.620833</td>\n",
              "      <td>0.613702</td>\n",
              "      <td>240</td>\n",
              "      <td>view_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.554622</td>\n",
              "      <td>0.602522</td>\n",
              "      <td>0.554605</td>\n",
              "      <td>0.538991</td>\n",
              "      <td>238</td>\n",
              "      <td>view_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  macro_precision  macro_recall  macro_f1    n          subgroup  \\\n",
              "0  0.641667         0.633554      0.641667  0.606805  120  background_floor   \n",
              "1  0.591667         0.665936      0.591667  0.580720  120   background_tree   \n",
              "2  0.576271         0.693355      0.577778  0.558293  118   background_wall   \n",
              "3  0.541667         0.590424      0.541667  0.536331  120  background_white   \n",
              "4  0.594142         0.646218      0.594124  0.578012  239        lighting_1   \n",
              "5  0.581590         0.625870      0.581838  0.575192  239        lighting_2   \n",
              "6  0.620833         0.668802      0.620833  0.613702  240            view_1   \n",
              "7  0.554622         0.602522      0.554605  0.538991  238            view_2   \n",
              "\n",
              "  note  \n",
              "0       \n",
              "1       \n",
              "2       \n",
              "3       \n",
              "4       \n",
              "5       \n",
              "6       \n",
              "7       "
            ]
          },
          "execution_count": 61,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_eval = data_transforms[\"val\"]\n",
        "\n",
        "df_subgroups = evaluate_all_subgroups(\n",
        "    model=resnet_aug_2,      \n",
        "    subgroups_root=\"subgroups\",\n",
        "    transform=transform_eval,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    print_results=True\n",
        ")\n",
        "\n",
        "df_subgroups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "id": "2ec4e5d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7819 Acc: 0.1833\n",
            "val Loss :1.7714 Acc: 0.2231\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7628 Acc: 0.2472\n",
            "val Loss :1.7466 Acc: 0.2948\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7417 Acc: 0.3244\n",
            "val Loss :1.7177 Acc: 0.3705\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.7198 Acc: 0.3853\n",
            "val Loss :1.6838 Acc: 0.4661\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.7001 Acc: 0.4156\n",
            "val Loss :1.6591 Acc: 0.5100\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.6747 Acc: 0.4720\n",
            "val Loss :1.6481 Acc: 0.5538\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.6547 Acc: 0.5042\n",
            "val Loss :1.5962 Acc: 0.6494\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.6329 Acc: 0.5310\n",
            "val Loss :1.5720 Acc: 0.6853\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.5996 Acc: 0.5468\n",
            "val Loss :1.5349 Acc: 0.6853\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.5713 Acc: 0.5661\n",
            "val Loss :1.4900 Acc: 0.7131\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.5485 Acc: 0.5646\n",
            "val Loss :1.4880 Acc: 0.7012\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.5181 Acc: 0.5780\n",
            "val Loss :1.4363 Acc: 0.7092\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.4747 Acc: 0.5948\n",
            "val Loss :1.3864 Acc: 0.7291\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.4485 Acc: 0.6117\n",
            "val Loss :1.3351 Acc: 0.7171\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.4096 Acc: 0.6290\n",
            "val Loss :1.2936 Acc: 0.7331\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.3905 Acc: 0.6137\n",
            "val Loss :1.2687 Acc: 0.7450\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.3473 Acc: 0.6290\n",
            "val Loss :1.2082 Acc: 0.7610\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.3177 Acc: 0.6434\n",
            "val Loss :1.1683 Acc: 0.7450\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.2998 Acc: 0.6246\n",
            "val Loss :1.1780 Acc: 0.7450\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.2653 Acc: 0.6389\n",
            "val Loss :1.1393 Acc: 0.7530\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.2291 Acc: 0.6379\n",
            "val Loss :1.0708 Acc: 0.7769\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :1.1995 Acc: 0.6493\n",
            "val Loss :1.0370 Acc: 0.7769\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :1.1682 Acc: 0.6592\n",
            "val Loss :1.0152 Acc: 0.7809\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :1.1403 Acc: 0.6731\n",
            "val Loss :0.9962 Acc: 0.7809\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :1.1335 Acc: 0.6592\n",
            "val Loss :0.9796 Acc: 0.7729\n",
            "\n",
            "Training complete in 4m 7s\n",
            "Best val Acc: 0.7809\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 mixed augmentations\n",
        "\n",
        "resnet_mixed = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_mixed.fc = nn.Linear(resnet_mixed.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_mixed = resnet_mixed.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_mixed.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_mixed, resnet_mixed_history = train_model(\n",
        "    model=resnet_mixed,\n",
        "    dataloaders=dataloaders_mix,\n",
        "    dataset_size=dataset_size_mix,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "id": "fbc5ce7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net Mix ID metrics\n",
            "{'accuracy': 0.7003891050583657, 'macro_precision': 0.698989898989899, 'macro_recall': 0.6796833367493339, 'macro_f1': 0.6804657519945038, 'n': 257}\n",
            "Res-Net Mix OOD metrics\n",
            "{'accuracy': 0.5774058577405857, 'macro_precision': 0.5740276025870498, 'macro_recall': 0.5764957264957264, 'macro_f1': 0.5737660243233843, 'n': 478}\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net Mix ID metrics\")\n",
        "test_metrics = evaluate_model_metrics(resnet_mixed, dataloaders_mix[\"test\"], device)\n",
        "print(test_metrics)\n",
        "print(\"Res-Net Mix OOD metrics\")\n",
        "ood_metrics = evaluate_model_metrics(resnet_mixed, dataloaders_mix[\"test_01\"], device)\n",
        "print(ood_metrics)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "id": "eea79756",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "background_floor     | n= 120 | acc=0.5667 | macroF1=0.5258\n",
            "background_tree      | n= 120 | acc=0.5333 | macroF1=0.5360\n",
            "background_wall      | n= 118 | acc=0.5763 | macroF1=0.5779\n",
            "background_white     | n= 120 | acc=0.6333 | macroF1=0.6346\n",
            "lighting_1           | n= 239 | acc=0.5900 | macroF1=0.5821\n",
            "lighting_2           | n= 239 | acc=0.5649 | macroF1=0.5665\n",
            "view_1               | n= 240 | acc=0.5958 | macroF1=0.5912\n",
            "view_2               | n= 238 | acc=0.5588 | macroF1=0.5544\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>accuracy</th>\n",
              "      <th>macro_precision</th>\n",
              "      <th>macro_recall</th>\n",
              "      <th>macro_f1</th>\n",
              "      <th>n</th>\n",
              "      <th>subgroup</th>\n",
              "      <th>note</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.554080</td>\n",
              "      <td>0.566667</td>\n",
              "      <td>0.525849</td>\n",
              "      <td>120</td>\n",
              "      <td>background_floor</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.585337</td>\n",
              "      <td>0.533333</td>\n",
              "      <td>0.535982</td>\n",
              "      <td>120</td>\n",
              "      <td>background_tree</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.576271</td>\n",
              "      <td>0.615405</td>\n",
              "      <td>0.573148</td>\n",
              "      <td>0.577850</td>\n",
              "      <td>118</td>\n",
              "      <td>background_wall</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.663034</td>\n",
              "      <td>0.633333</td>\n",
              "      <td>0.634642</td>\n",
              "      <td>120</td>\n",
              "      <td>background_white</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.589958</td>\n",
              "      <td>0.582479</td>\n",
              "      <td>0.588889</td>\n",
              "      <td>0.582054</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>0.564854</td>\n",
              "      <td>0.571248</td>\n",
              "      <td>0.564103</td>\n",
              "      <td>0.566461</td>\n",
              "      <td>239</td>\n",
              "      <td>lighting_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0.595833</td>\n",
              "      <td>0.596017</td>\n",
              "      <td>0.595833</td>\n",
              "      <td>0.591234</td>\n",
              "      <td>240</td>\n",
              "      <td>view_1</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>0.558824</td>\n",
              "      <td>0.557616</td>\n",
              "      <td>0.557675</td>\n",
              "      <td>0.554360</td>\n",
              "      <td>238</td>\n",
              "      <td>view_2</td>\n",
              "      <td></td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   accuracy  macro_precision  macro_recall  macro_f1    n          subgroup  \\\n",
              "0  0.566667         0.554080      0.566667  0.525849  120  background_floor   \n",
              "1  0.533333         0.585337      0.533333  0.535982  120   background_tree   \n",
              "2  0.576271         0.615405      0.573148  0.577850  118   background_wall   \n",
              "3  0.633333         0.663034      0.633333  0.634642  120  background_white   \n",
              "4  0.589958         0.582479      0.588889  0.582054  239        lighting_1   \n",
              "5  0.564854         0.571248      0.564103  0.566461  239        lighting_2   \n",
              "6  0.595833         0.596017      0.595833  0.591234  240            view_1   \n",
              "7  0.558824         0.557616      0.557675  0.554360  238            view_2   \n",
              "\n",
              "  note  \n",
              "0       \n",
              "1       \n",
              "2       \n",
              "3       \n",
              "4       \n",
              "5       \n",
              "6       \n",
              "7       "
            ]
          },
          "execution_count": 70,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "transform_eval = data_transforms[\"val\"]\n",
        "\n",
        "df_subgroups = evaluate_all_subgroups(\n",
        "    model=resnet_mixed,      \n",
        "    subgroups_root=\"subgroups\",\n",
        "    transform=transform_eval,\n",
        "    device=device,\n",
        "    batch_size=32,\n",
        "    num_workers=4,\n",
        "    print_results=True\n",
        ")\n",
        "\n",
        "df_subgroups\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "801b7774",
      "metadata": {},
      "outputs": [],
      "source": [
        "del resnet\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831df688",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Efficientnet\n",
        "\n",
        "efficient_net = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT) #Load pretrained model\n",
        "\n",
        "in_features = efficient_net.classifier[1].in_features #Replace last classifier layer\n",
        "efficient_net.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "efficient_net = efficient_net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #define loss\n",
        "optimizer = optim.SGD(efficient_net.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "efficient_net, efficient_net_history = train_model(\n",
        "    model=efficient_net,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf9bbe5",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Efficient_net test metrics\")\n",
        "evaluate_model(efficient_net, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"Efficient_net test_01 metrics\")\n",
        "evaluate_model(efficient_net, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326f52d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "del efficient_net\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80405db1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#densenet\n",
        "\n",
        "densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT) #Load pretrained model\n",
        "densenet.classifier = nn.Linear(densenet.classifier.in_features, num_classes)\n",
        "densenet = densenet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #define loss\n",
        "optimizer = optim.SGD(densenet.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "densenet, densenet_history = train_model(\n",
        "    model=densenet,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a8cca041",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "densenet test metrics\n",
            "Accuracy:  0.9066\n",
            "Precision: 0.9172\n",
            "Recall:    0.8697\n",
            "F1-score:  0.8849\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.95      0.98      0.96        41\n",
            "       glass       0.85      0.86      0.85        51\n",
            "       metal       0.84      0.88      0.86        41\n",
            "       paper       0.97      0.98      0.98        60\n",
            "     plastic       0.90      0.92      0.91        49\n",
            "       trash       1.00      0.60      0.75        15\n",
            "\n",
            "    accuracy                           0.91       257\n",
            "   macro avg       0.92      0.87      0.88       257\n",
            "weighted avg       0.91      0.91      0.90       257\n",
            "\n",
            "densenet test_01 metrics\n",
            "Accuracy:  0.4270\n",
            "Precision: 0.8017\n",
            "Recall:    0.4294\n",
            "F1-score:  0.4185\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.24      1.00      0.39        15\n",
            "       glass       0.82      0.64      0.72        14\n",
            "       metal       1.00      0.27      0.42        15\n",
            "       paper       0.75      0.40      0.52        15\n",
            "     plastic       1.00      0.20      0.33        15\n",
            "       trash       1.00      0.07      0.12        15\n",
            "\n",
            "    accuracy                           0.43        89\n",
            "   macro avg       0.80      0.43      0.42        89\n",
            "weighted avg       0.80      0.43      0.42        89\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"densenet test metrics\")\n",
        "evaluate_model(densenet, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"densenet test_01 metrics\")\n",
        "evaluate_model(densenet, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4a6321",
      "metadata": {},
      "outputs": [],
      "source": [
        "del densenet\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "95a8bb25",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1.6%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /home/sera/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.5678 Acc: 0.4255\n",
            "val Loss :1.3140 Acc: 0.5817\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.1305 Acc: 0.7083\n",
            "val Loss :0.9192 Acc: 0.7849\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :0.8742 Acc: 0.7940\n",
            "val Loss :0.7776 Acc: 0.8247\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :0.7217 Acc: 0.8143\n",
            "val Loss :0.6549 Acc: 0.8327\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :0.6075 Acc: 0.8455\n",
            "val Loss :0.5063 Acc: 0.8805\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :0.5324 Acc: 0.8623\n",
            "val Loss :1.1671 Acc: 0.4940\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :0.5099 Acc: 0.8613\n",
            "val Loss :0.4446 Acc: 0.8805\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :0.4330 Acc: 0.8920\n",
            "val Loss :0.4732 Acc: 0.8845\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :0.3919 Acc: 0.9000\n",
            "val Loss :0.3862 Acc: 0.8964\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :0.3516 Acc: 0.9099\n",
            "val Loss :0.5459 Acc: 0.8406\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :0.3484 Acc: 0.9104\n",
            "val Loss :0.2957 Acc: 0.9323\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :0.2991 Acc: 0.9292\n",
            "val Loss :0.5274 Acc: 0.8247\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :0.2851 Acc: 0.9247\n",
            "val Loss :0.2701 Acc: 0.9243\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :0.2624 Acc: 0.9336\n",
            "val Loss :0.2840 Acc: 0.9203\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :0.2420 Acc: 0.9376\n",
            "val Loss :0.2537 Acc: 0.9243\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :0.2219 Acc: 0.9539\n",
            "val Loss :0.4267 Acc: 0.8685\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :0.2118 Acc: 0.9485\n",
            "val Loss :0.2438 Acc: 0.9243\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :0.1969 Acc: 0.9554\n",
            "val Loss :0.2688 Acc: 0.9124\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :0.1854 Acc: 0.9574\n",
            "val Loss :0.2439 Acc: 0.9124\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :0.1709 Acc: 0.9663\n",
            "val Loss :0.2548 Acc: 0.9163\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :0.1546 Acc: 0.9673\n",
            "val Loss :0.2191 Acc: 0.9402\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.1474 Acc: 0.9688\n",
            "val Loss :0.2188 Acc: 0.9283\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.1335 Acc: 0.9733\n",
            "val Loss :0.2186 Acc: 0.9203\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.1223 Acc: 0.9807\n",
            "val Loss :0.2335 Acc: 0.9203\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.1242 Acc: 0.9767\n",
            "val Loss :0.1869 Acc: 0.9482\n",
            "\n",
            "Training complete in 7m 27s\n",
            "Best val Acc: 0.9482\n"
          ]
        }
      ],
      "source": [
        "# ConvNeXt-Base\n",
        "convnext_base = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "in_features = convnext_base.classifier[2].in_features\n",
        "convnext_base.classifier[2] = nn.Linear(in_features, num_classes)\n",
        "convnext_base = convnext_base.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(convnext_base.parameters(), lr=0.001)  \n",
        "\n",
        "convnext_base, convnext_base_history = train_model(\n",
        "    model=convnext_base,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0be71614",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNext test metrics\n",
            "Accuracy:  0.9183\n",
            "Precision: 0.9298\n",
            "Recall:    0.8988\n",
            "F1-score:  0.9091\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.95      0.98      0.96        41\n",
            "       glass       0.95      0.82      0.88        51\n",
            "       metal       0.87      0.95      0.91        41\n",
            "       paper       0.97      0.95      0.96        60\n",
            "     plastic       0.84      0.96      0.90        49\n",
            "       trash       1.00      0.73      0.85        15\n",
            "\n",
            "    accuracy                           0.92       257\n",
            "   macro avg       0.93      0.90      0.91       257\n",
            "weighted avg       0.92      0.92      0.92       257\n",
            "\n",
            "convnext_base test_01 metrics\n",
            "Accuracy:  0.3708\n",
            "Precision: 0.5576\n",
            "Recall:    0.3698\n",
            "F1-score:  0.3517\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.23      1.00      0.38        15\n",
            "       glass       1.00      0.29      0.44        14\n",
            "       metal       0.83      0.33      0.48        15\n",
            "       paper       0.83      0.33      0.48        15\n",
            "     plastic       0.44      0.27      0.33        15\n",
            "       trash       0.00      0.00      0.00        15\n",
            "\n",
            "    accuracy                           0.37        89\n",
            "   macro avg       0.56      0.37      0.35        89\n",
            "weighted avg       0.55      0.37      0.35        89\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ConvNext test metrics\")\n",
        "evaluate_model(convnext_base, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"convnext_base test_01 metrics\")\n",
        "evaluate_model(convnext_base, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a904d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "del convnext_base\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "c8aef1f1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "LIGHTING\n",
            "lighting\n",
            "1    239\n",
            "2    239\n",
            "Name: count, dtype: int64\n",
            "\n",
            "BACKGROUND\n",
            "background\n",
            "tree     120\n",
            "white    120\n",
            "floor    120\n",
            "wall     118\n",
            "Name: count, dtype: int64\n",
            "\n",
            "VIEW\n",
            "view\n",
            "1    240\n",
            "2    238\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"subgroups.csv\")\n",
        "\n",
        "for col in [\"lighting\", \"background\", \"view\"]:\n",
        "    print(f\"\\n{col.upper()}\")\n",
        "    print(df[col].value_counts())\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (thesis venv)",
      "language": "python",
      "name": "thesis-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
