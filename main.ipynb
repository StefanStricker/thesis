{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "22d20b92",
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import torch\n",
        "from torchvision import datasets, models, transforms\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "from pathlib import Path\n",
        "import time\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "d0ba08e5",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'2.9.0+cu128'"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.__version__"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "b7612515",
      "metadata": {},
      "outputs": [],
      "source": [
        "input_path = Path(\"data/datasets/trashnet_01/\")\n",
        "input_path2 = Path(\"data/datasets/self-collected-01/\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "e1d693c3",
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "\n",
        "seed = 16\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "e4dbb0bd",
      "metadata": {},
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "6821f7d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Classes: ['cardboard', 'glass', 'metal', 'paper', 'plastic', 'trash']\n",
            "Train counts: Counter({3: 475, 1: 400, 4: 385, 2: 328, 0: 322, 5: 109})\n",
            "Class weights: tensor([0.8270, 0.6657, 0.8119, 0.5606, 0.6917, 2.4431], device='cuda:0')\n"
          ]
        }
      ],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def compute_class_weights_from_imagefolder(train_root, num_classes):\n",
        "    ds = datasets.ImageFolder(train_root, transform=transforms.ToTensor())\n",
        "    counts = Counter(ds.targets)\n",
        "    total = sum(counts.values())\n",
        "\n",
        "    weights = [total / counts[i] for i in range(num_classes)]\n",
        "    weights = torch.tensor(weights, dtype=torch.float)\n",
        "    weights = weights / weights.sum() * num_classes \n",
        "    return weights, ds.classes, counts\n",
        "\n",
        "tmp_ds = datasets.ImageFolder(input_path / \"train\")\n",
        "num_classes = len(tmp_ds.classes)\n",
        "\n",
        "class_weights, class_names, train_counts = compute_class_weights_from_imagefolder(\n",
        "    input_path / \"train\",\n",
        "    num_classes\n",
        ")\n",
        "\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "criterion_weighted = torch.nn.CrossEntropyLoss(weight=class_weights)\n",
        "\n",
        "print(\"Classes:\", class_names)\n",
        "print(\"Train counts:\", train_counts)\n",
        "print(\"Class weights:\", class_weights)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "55b330fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "# baseline data transforms\n",
        "\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "\n",
        "data_transforms = {\n",
        "\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),  \n",
        "        transforms.RandomHorizontalFlip(), \n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),    \n",
        "}\n",
        "\n",
        "image_datasets = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms[\"val\"]),\n",
        "\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms[\"val\"])\n",
        "}\n",
        "\n",
        "\n",
        "dataloaders = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}   \n",
        "\n",
        "\n",
        "dataset_size = {\n",
        "    \"train\": len(image_datasets[\"train\"]),\n",
        "    \"val\": len(image_datasets[\"val\"]),\n",
        "    \"test_01\": len(image_datasets[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes = len((image_datasets[\"train\"]).classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "50c0f428",
      "metadata": {},
      "outputs": [],
      "source": [
        "#geometric augmentation\n",
        "\n",
        "data_transforms_geo = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_geo = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_geo[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_geo[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_geo[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_geo[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_geo = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_geo[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_geo[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_geo[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_geo[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_geo = {\n",
        "    \"train\": len(image_datasets_geo[\"train\"]),\n",
        "    \"val\": len(image_datasets_geo[\"val\"]),\n",
        "    \"test\": len(image_datasets_geo[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_geo[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_geo = len(image_datasets_geo[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c86eda2e",
      "metadata": {},
      "outputs": [],
      "source": [
        "#photometric\n",
        " \n",
        "data_transforms_photo = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
        "        transforms.RandomGrayscale(p=0.1),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_photo = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_photo[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_photo[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_photo[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_photo[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_photo = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_photo[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_photo[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_photo[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_photo[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_photo = {\n",
        "    \"train\": len(image_datasets_photo[\"train\"]),\n",
        "    \"val\": len(image_datasets_photo[\"val\"]),\n",
        "    \"test\": len(image_datasets_photo[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_photo[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_photo = len(image_datasets_photo[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1a6de36",
      "metadata": {},
      "outputs": [],
      "source": [
        "#aug mix\n",
        " \n",
        "data_transforms_mix = {\n",
        "    \"train\": transforms.Compose([\n",
        "        transforms.RandomResizedCrop(224),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.RandomRotation(15),\n",
        "        transforms.RandomAffine(degrees=0, translate=(0.05, 0.05), scale=(0.9, 1.1), shear=5),\n",
        "        transforms.ColorJitter(brightness=0.25, contrast=0.25, saturation=0.25, hue=0.1),\n",
        "        transforms.GaussianBlur(kernel_size=3, sigma=(0.1, 2.0)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize,\n",
        "        transforms.RandomErasing(p=0.5, scale=(0.02, 0.2), ratio=(0.3, 3.3), value='random')\n",
        "    ]),\n",
        "    \"val\": transforms.Compose([\n",
        "        transforms.Resize((224,224)),\n",
        "        transforms.ToTensor(),\n",
        "        normalize\n",
        "    ]), \n",
        "}\n",
        "\n",
        "image_datasets_mix = {\n",
        "    \"train\": datasets.ImageFolder(input_path / \"train\", data_transforms_mix[\"train\"]),\n",
        "    \"val\": datasets.ImageFolder(input_path / \"val\", data_transforms_mix[\"val\"]),\n",
        "    \"test\": datasets.ImageFolder(input_path / \"test\", data_transforms_mix[\"val\"]),\n",
        "    \"test_01\": datasets.ImageFolder(input_path2, data_transforms_mix[\"val\"])\n",
        "}\n",
        "\n",
        "dataloaders_mix = {\n",
        "    \"train\": torch.utils.data.DataLoader(image_datasets_mix[\"train\"], batch_size=32, shuffle=True, num_workers=4),\n",
        "    \"val\": torch.utils.data.DataLoader(image_datasets_mix[\"val\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test\": torch.utils.data.DataLoader(image_datasets_mix[\"test\"], batch_size=32, shuffle=False, num_workers=4),\n",
        "    \"test_01\": torch.utils.data.DataLoader(image_datasets_mix[\"test_01\"], batch_size=32, shuffle=False, num_workers=4)\n",
        "}\n",
        "\n",
        "dataset_size_mix = {\n",
        "    \"train\": len(image_datasets_mix[\"train\"]),\n",
        "    \"val\": len(image_datasets_mix[\"val\"]),\n",
        "    \"test\": len(image_datasets_mix[\"test\"]),\n",
        "    \"test_01\": len(image_datasets_mix[\"test_01\"])\n",
        "}\n",
        "\n",
        "num_classes_mix = len(image_datasets_mix[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "abaa0b3f",
      "metadata": {},
      "outputs": [],
      "source": [
        "#MixUp function\n",
        "\n",
        "def apply_mixup(inputs, labels, alpha=0.4):\n",
        "    if alpha <= 0:\n",
        "        return inputs, labels, labels, 1.0\n",
        "\n",
        "    lam = np.random.beta(alpha, alpha)\n",
        "    batch_size = inputs.size(0)\n",
        "    index = torch.randperm(batch_size, device=inputs.device)\n",
        "\n",
        "    mixed_inputs = lam * inputs + (1 - lam) * inputs[index]\n",
        "    labels_a = labels\n",
        "    labels_b = labels[index]\n",
        "    return mixed_inputs, labels_a, labels_b, lam"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "b14f42c8",
      "metadata": {},
      "outputs": [],
      "source": [
        "def train_model(model, dataloaders, dataset_size, device, criterion, optimizer, num_epochs = 25, use_mixup=False, mixup_prob=0.5, mixup_alpha=0.4):\n",
        "    since = time.time()\n",
        "\n",
        "    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "    best_acc = 0.0\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"val_loss\": [],\n",
        "        \"val_acc\": [],\n",
        "    }\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
        "        print(\"-\" * 10)\n",
        "\n",
        "        for phase in [\"train\", \"val\"]:\n",
        "            if phase == \"train\":\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "\n",
        "            running_loss = 0.0\n",
        "            running_corrects = 0.0\n",
        "\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs = inputs.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                with torch.set_grad_enabled(phase == \"train\"):\n",
        "\n",
        "                        if phase == \"train\" and use_mixup and (np.random.rand() < mixup_prob):\n",
        "                            inputs_mixed, y_a, y_b, lam = apply_mixup(inputs, labels, alpha=mixup_alpha)\n",
        "\n",
        "                            outputs = model(inputs_mixed)\n",
        "                            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                            loss = lam * criterion(outputs, y_a) + (1 - lam) * criterion(outputs, y_b)\n",
        "\n",
        "                            running_corrects += (float(lam) * torch.sum(preds == y_a).item() +\n",
        "                                                (1 - float(lam)) * torch.sum(preds == y_b).item())\n",
        "                        else:\n",
        "                            outputs = model(inputs)\n",
        "                            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "                            loss = criterion(outputs, labels)\n",
        "                            running_corrects += torch.sum(preds == labels).item()\n",
        "\n",
        "                        if phase == \"train\":\n",
        "                            loss.backward()\n",
        "                            optimizer.step()\n",
        "                        \n",
        "                running_loss += loss.item() * inputs.size(0)      \n",
        "\n",
        "            epoch_loss = running_loss / dataset_size[phase]\n",
        "            epoch_acc = running_corrects / dataset_size[phase]\n",
        "\n",
        "            print(f\"{phase} Loss :{epoch_loss:.4f} Acc: {epoch_acc:.4f}\")     \n",
        "\n",
        "            if phase == \"train\":\n",
        "                history[\"train_loss\"].append(epoch_loss)    \n",
        "                history[\"train_acc\"].append(epoch_acc)   \n",
        "            else:\n",
        "                history[\"val_loss\"].append(epoch_loss)  \n",
        "                history[\"val_acc\"].append(epoch_acc)  \n",
        "\n",
        "            if phase == \"val\" and epoch_acc > best_acc:\n",
        "                best_acc = epoch_acc\n",
        "                best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        print()              \n",
        "\n",
        "    time_elapsed = time.time() - since\n",
        "    print(f\"Training complete in {time_elapsed // 60:.0f}m {time_elapsed % 60:.0f}s\")\n",
        "    print(f\"Best val Acc: {best_acc:.4f}\")\n",
        "\n",
        "    model.load_state_dict(best_model_wts)\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "51cb155f",
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.metrics import classification_report, accuracy_score, precision_recall_fscore_support\n",
        "\n",
        "def evaluate_model(model, dataloader, device, class_names):\n",
        "    model.eval()\n",
        "    preds_list, labels_list = [], []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs = inputs.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            _, preds = torch.max(outputs, 1)\n",
        "\n",
        "            preds_list.extend(preds.cpu().numpy())\n",
        "            labels_list.extend(labels.cpu().numpy())\n",
        "\n",
        "    acc = accuracy_score(labels_list, preds_list)\n",
        "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
        "        labels_list, preds_list, average='macro', zero_division=0\n",
        "    )\n",
        "\n",
        "    print(f\"Accuracy:  {acc:.4f}\")\n",
        "    print(f\"Precision: {precision:.4f}\")\n",
        "    print(f\"Recall:    {recall:.4f}\")\n",
        "    print(f\"F1-score:  {f1:.4f}\")\n",
        "    print(\"\\nPer-class breakdown:\")\n",
        "    print(classification_report(labels_list, preds_list, target_names=class_names, zero_division=0))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "74293688",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7937 Acc: 0.1446\n",
            "val Loss :1.7645 Acc: 0.1952\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7462 Acc: 0.2704\n",
            "val Loss :1.7145 Acc: 0.3466\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7054 Acc: 0.3938\n",
            "val Loss :1.6590 Acc: 0.4502\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6549 Acc: 0.5067\n",
            "val Loss :1.6124 Acc: 0.5259\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6108 Acc: 0.5577\n",
            "val Loss :1.5549 Acc: 0.5976\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.5666 Acc: 0.5844\n",
            "val Loss :1.4945 Acc: 0.6534\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5175 Acc: 0.6176\n",
            "val Loss :1.4437 Acc: 0.6773\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.4701 Acc: 0.6241\n",
            "val Loss :1.3876 Acc: 0.6813\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.4128 Acc: 0.6528\n",
            "val Loss :1.3269 Acc: 0.7211\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.3689 Acc: 0.6582\n",
            "val Loss :1.2550 Acc: 0.7331\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.3216 Acc: 0.6474\n",
            "val Loss :1.2577 Acc: 0.7410\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.2607 Acc: 0.6880\n",
            "val Loss :1.1700 Acc: 0.7570\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.2139 Acc: 0.6771\n",
            "val Loss :1.1063 Acc: 0.7450\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.1754 Acc: 0.7073\n",
            "val Loss :1.0640 Acc: 0.7610\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.1316 Acc: 0.6909\n",
            "val Loss :1.0049 Acc: 0.7530\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.0971 Acc: 0.6825\n",
            "val Loss :0.9667 Acc: 0.7809\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.0515 Acc: 0.7043\n",
            "val Loss :0.9195 Acc: 0.7809\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.0240 Acc: 0.7093\n",
            "val Loss :0.9038 Acc: 0.7809\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :0.9964 Acc: 0.6989\n",
            "val Loss :0.8277 Acc: 0.7849\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :0.9659 Acc: 0.7132\n",
            "val Loss :0.7920 Acc: 0.8088\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :0.9301 Acc: 0.7202\n",
            "val Loss :0.8210 Acc: 0.7968\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.8989 Acc: 0.7345\n",
            "val Loss :0.7326 Acc: 0.8088\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.8821 Acc: 0.7296\n",
            "val Loss :0.7432 Acc: 0.8247\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.8592 Acc: 0.7380\n",
            "val Loss :0.6668 Acc: 0.8207\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.8488 Acc: 0.7410\n",
            "val Loss :0.6568 Acc: 0.8247\n",
            "\n",
            "Training complete in 4m 7s\n",
            "Best val Acc: 0.8247\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50\n",
        "\n",
        "resnet = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet.fc = nn.Linear(resnet.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet = resnet.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet, resnet_history = train_model(\n",
        "    model=resnet,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "fbace900",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net test metrics\n",
            "Accuracy:  0.7821\n",
            "Precision: 0.7506\n",
            "Recall:    0.7623\n",
            "F1-score:  0.7524\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.92      0.83      0.87        41\n",
            "       glass       0.76      0.76      0.76        51\n",
            "       metal       0.70      0.63      0.67        41\n",
            "       paper       0.90      0.88      0.89        60\n",
            "     plastic       0.76      0.80      0.78        49\n",
            "       trash       0.45      0.67      0.54        15\n",
            "\n",
            "    accuracy                           0.78       257\n",
            "   macro avg       0.75      0.76      0.75       257\n",
            "weighted avg       0.79      0.78      0.79       257\n",
            "\n",
            "Res-Net test_01 metrics\n",
            "Accuracy:  0.5000\n",
            "Precision: 0.5708\n",
            "Recall:    0.4957\n",
            "F1-score:  0.4904\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.67      0.39      0.49        31\n",
            "       glass       0.85      0.64      0.73        44\n",
            "       metal       0.59      0.38      0.46        45\n",
            "       paper       0.37      0.80      0.50        44\n",
            "     plastic       0.42      0.60      0.50        45\n",
            "       trash       0.53      0.18      0.27        45\n",
            "\n",
            "    accuracy                           0.50       254\n",
            "   macro avg       0.57      0.50      0.49       254\n",
            "weighted avg       0.57      0.50      0.49       254\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net test metrics\")\n",
        "evaluate_model(resnet, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"Res-Net test_01 metrics\")\n",
        "evaluate_model(resnet, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "e938db82",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7624 Acc: 0.2888\n",
            "val Loss :1.7141 Acc: 0.4183\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7275 Acc: 0.3809\n",
            "val Loss :1.6665 Acc: 0.5060\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.6840 Acc: 0.4651\n",
            "val Loss :1.6227 Acc: 0.5976\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6401 Acc: 0.5374\n",
            "val Loss :1.5746 Acc: 0.6335\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6068 Acc: 0.5627\n",
            "val Loss :1.5240 Acc: 0.7052\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.5667 Acc: 0.5914\n",
            "val Loss :1.4995 Acc: 0.7171\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5226 Acc: 0.6092\n",
            "val Loss :1.4243 Acc: 0.7450\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.4782 Acc: 0.6275\n",
            "val Loss :1.3623 Acc: 0.7490\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.4424 Acc: 0.6216\n",
            "val Loss :1.3289 Acc: 0.7251\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.3977 Acc: 0.6369\n",
            "val Loss :1.2785 Acc: 0.7610\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.3550 Acc: 0.6394\n",
            "val Loss :1.2231 Acc: 0.7450\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.3066 Acc: 0.6622\n",
            "val Loss :1.1645 Acc: 0.7729\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.2713 Acc: 0.6543\n",
            "val Loss :1.1105 Acc: 0.7530\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.2313 Acc: 0.6686\n",
            "val Loss :1.0787 Acc: 0.7649\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.1921 Acc: 0.6538\n",
            "val Loss :1.0218 Acc: 0.7928\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.1676 Acc: 0.6528\n",
            "val Loss :0.9738 Acc: 0.7729\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.1286 Acc: 0.6602\n",
            "val Loss :0.9352 Acc: 0.8008\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.0885 Acc: 0.6751\n",
            "val Loss :0.9745 Acc: 0.7849\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.0436 Acc: 0.6969\n",
            "val Loss :0.9095 Acc: 0.7888\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.0295 Acc: 0.6984\n",
            "val Loss :0.8386 Acc: 0.7968\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.0092 Acc: 0.7073\n",
            "val Loss :0.8398 Acc: 0.8008\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.9704 Acc: 0.7093\n",
            "val Loss :0.7671 Acc: 0.8008\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.9593 Acc: 0.7068\n",
            "val Loss :0.7397 Acc: 0.8048\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.9259 Acc: 0.7117\n",
            "val Loss :0.7051 Acc: 0.8207\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.9052 Acc: 0.7132\n",
            "val Loss :0.6944 Acc: 0.8048\n",
            "\n",
            "Training complete in 4m 7s\n",
            "Best val Acc: 0.8207\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 photo augmentations\n",
        "\n",
        "resnet_aug_1 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_aug_1.fc = nn.Linear(resnet_aug_1.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_aug_1 = resnet_aug_1.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_aug_1.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_aug_1, resnet_history_aug_1 = train_model(\n",
        "    model=resnet_aug_1,\n",
        "    dataloaders=dataloaders_photo,\n",
        "    dataset_size=dataset_size_photo,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "id": "34c036c3",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net test metrics\n",
            "Accuracy:  0.7665\n",
            "Precision: 0.7404\n",
            "Recall:    0.7385\n",
            "F1-score:  0.7299\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.94      0.76      0.84        41\n",
            "       glass       0.73      0.84      0.78        51\n",
            "       metal       0.68      0.63      0.66        41\n",
            "       paper       0.87      0.88      0.88        60\n",
            "     plastic       0.88      0.71      0.79        49\n",
            "       trash       0.35      0.60      0.44        15\n",
            "\n",
            "    accuracy                           0.77       257\n",
            "   macro avg       0.74      0.74      0.73       257\n",
            "weighted avg       0.79      0.77      0.77       257\n",
            "\n",
            "Res-Net test_01 metrics\n",
            "Accuracy:  0.5039\n",
            "Precision: 0.5215\n",
            "Recall:    0.5122\n",
            "F1-score:  0.5032\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.50      0.65      0.56        31\n",
            "       glass       0.70      0.68      0.69        44\n",
            "       metal       0.61      0.31      0.41        45\n",
            "       paper       0.43      0.59      0.50        44\n",
            "     plastic       0.40      0.47      0.43        45\n",
            "       trash       0.50      0.38      0.43        45\n",
            "\n",
            "    accuracy                           0.50       254\n",
            "   macro avg       0.52      0.51      0.50       254\n",
            "weighted avg       0.52      0.50      0.50       254\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net test metrics\")\n",
        "evaluate_model(resnet_aug_1, dataloaders_photo[\"test\"], device, image_datasets_photo[\"train\"].classes)\n",
        "print(\"Res-Net test_01 metrics\")\n",
        "evaluate_model(resnet_aug_1, dataloaders_photo[\"test_01\"], device, image_datasets_photo[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "416b1b32",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7687 Acc: 0.2364\n",
            "val Loss :1.7330 Acc: 0.3307\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7448 Acc: 0.2941\n",
            "val Loss :1.7035 Acc: 0.3904\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7173 Acc: 0.3789\n",
            "val Loss :1.6710 Acc: 0.4861\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.6844 Acc: 0.4638\n",
            "val Loss :1.6158 Acc: 0.5976\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.6569 Acc: 0.4786\n",
            "val Loss :1.5826 Acc: 0.6295\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.6345 Acc: 0.5125\n",
            "val Loss :1.5504 Acc: 0.6574\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.5843 Acc: 0.5536\n",
            "val Loss :1.4939 Acc: 0.7052\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.5665 Acc: 0.5399\n",
            "val Loss :1.4707 Acc: 0.7131\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.5434 Acc: 0.5343\n",
            "val Loss :1.4391 Acc: 0.7012\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.4953 Acc: 0.5722\n",
            "val Loss :1.4140 Acc: 0.7131\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.4756 Acc: 0.5737\n",
            "val Loss :1.3758 Acc: 0.7251\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.4449 Acc: 0.5681\n",
            "val Loss :1.3173 Acc: 0.7450\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.3934 Acc: 0.6067\n",
            "val Loss :1.2879 Acc: 0.7251\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.4051 Acc: 0.5857\n",
            "val Loss :1.3017 Acc: 0.7570\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.3336 Acc: 0.6180\n",
            "val Loss :1.2063 Acc: 0.7450\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.3295 Acc: 0.6001\n",
            "val Loss :1.1661 Acc: 0.7530\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.2886 Acc: 0.6188\n",
            "val Loss :1.1113 Acc: 0.7928\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.2807 Acc: 0.6023\n",
            "val Loss :1.0714 Acc: 0.7729\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.2280 Acc: 0.6269\n",
            "val Loss :1.0492 Acc: 0.7610\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.2205 Acc: 0.6197\n",
            "val Loss :1.0570 Acc: 0.7849\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.1647 Acc: 0.6494\n",
            "val Loss :0.9846 Acc: 0.7769\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :1.1769 Acc: 0.6221\n",
            "val Loss :0.9756 Acc: 0.7888\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :1.1451 Acc: 0.6409\n",
            "val Loss :0.9749 Acc: 0.7888\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :1.1287 Acc: 0.6434\n",
            "val Loss :0.9419 Acc: 0.7968\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :1.1361 Acc: 0.6365\n",
            "val Loss :0.8910 Acc: 0.7928\n",
            "\n",
            "Training complete in 4m 17s\n",
            "Best val Acc: 0.7968\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 geo augmentations\n",
        "\n",
        "resnet_aug_2 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_aug_2.fc = nn.Linear(resnet_aug_2.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_aug_2 = resnet_aug_2.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_aug_2.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_aug_2, resnet_history_aug_2 = train_model(\n",
        "    model=resnet_aug_2,\n",
        "    dataloaders=dataloaders_geo,\n",
        "    dataset_size=dataset_size_geo,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "id": "cb2bd59f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net test metrics\n",
            "Accuracy:  0.7276\n",
            "Precision: 0.7162\n",
            "Recall:    0.7079\n",
            "F1-score:  0.7006\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.90      0.66      0.76        41\n",
            "       glass       0.71      0.80      0.75        51\n",
            "       metal       0.57      0.76      0.65        41\n",
            "       paper       0.83      0.82      0.82        60\n",
            "     plastic       0.86      0.61      0.71        49\n",
            "       trash       0.43      0.60      0.50        15\n",
            "\n",
            "    accuracy                           0.73       257\n",
            "   macro avg       0.72      0.71      0.70       257\n",
            "weighted avg       0.76      0.73      0.73       257\n",
            "\n",
            "Res-Net test_01 metrics\n",
            "Accuracy:  0.5827\n",
            "Precision: 0.6410\n",
            "Recall:    0.5871\n",
            "F1-score:  0.5886\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.95      0.65      0.77        31\n",
            "       glass       0.62      0.75      0.68        44\n",
            "       metal       0.49      0.62      0.55        45\n",
            "       paper       0.58      0.73      0.65        44\n",
            "     plastic       0.42      0.47      0.44        45\n",
            "       trash       0.78      0.31      0.44        45\n",
            "\n",
            "    accuracy                           0.58       254\n",
            "   macro avg       0.64      0.59      0.59       254\n",
            "weighted avg       0.62      0.58      0.58       254\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net test metrics\")\n",
        "evaluate_model(resnet_aug_2, dataloaders_geo[\"test\"], device, image_datasets_geo[\"train\"].classes)\n",
        "print(\"Res-Net test_01 metrics\")\n",
        "evaluate_model(resnet_aug_2, dataloaders_geo[\"test_01\"], device, image_datasets_geo[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "id": "2ec4e5d6",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.7857 Acc: 0.1877\n",
            "val Loss :1.7574 Acc: 0.2948\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.7661 Acc: 0.2452\n",
            "val Loss :1.7322 Acc: 0.3625\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :1.7461 Acc: 0.3284\n",
            "val Loss :1.7053 Acc: 0.4143\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :1.7300 Acc: 0.3784\n",
            "val Loss :1.6772 Acc: 0.4980\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :1.7057 Acc: 0.4185\n",
            "val Loss :1.6590 Acc: 0.5418\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :1.6879 Acc: 0.4532\n",
            "val Loss :1.6206 Acc: 0.5817\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :1.6645 Acc: 0.4993\n",
            "val Loss :1.5856 Acc: 0.6335\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :1.6404 Acc: 0.4968\n",
            "val Loss :1.5562 Acc: 0.6534\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :1.6237 Acc: 0.5106\n",
            "val Loss :1.5323 Acc: 0.6653\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :1.5884 Acc: 0.5448\n",
            "val Loss :1.4889 Acc: 0.6972\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :1.5579 Acc: 0.5661\n",
            "val Loss :1.4621 Acc: 0.6972\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :1.5298 Acc: 0.5656\n",
            "val Loss :1.4247 Acc: 0.7131\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :1.5026 Acc: 0.5666\n",
            "val Loss :1.3821 Acc: 0.7052\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :1.4588 Acc: 0.5978\n",
            "val Loss :1.3375 Acc: 0.7251\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :1.4388 Acc: 0.5924\n",
            "val Loss :1.3075 Acc: 0.7490\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :1.4056 Acc: 0.6062\n",
            "val Loss :1.2621 Acc: 0.7490\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :1.3605 Acc: 0.6261\n",
            "val Loss :1.2284 Acc: 0.7649\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :1.3394 Acc: 0.6043\n",
            "val Loss :1.1653 Acc: 0.7490\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :1.2982 Acc: 0.6171\n",
            "val Loss :1.1382 Acc: 0.7689\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :1.2594 Acc: 0.6261\n",
            "val Loss :1.0928 Acc: 0.7689\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :1.2387 Acc: 0.6404\n",
            "val Loss :1.0557 Acc: 0.7769\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :1.2129 Acc: 0.6434\n",
            "val Loss :1.0434 Acc: 0.7769\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :1.1749 Acc: 0.6459\n",
            "val Loss :1.0014 Acc: 0.7928\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :1.1637 Acc: 0.6503\n",
            "val Loss :0.9534 Acc: 0.7809\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :1.1335 Acc: 0.6508\n",
            "val Loss :0.9170 Acc: 0.7809\n",
            "\n",
            "Training complete in 4m 16s\n",
            "Best val Acc: 0.7928\n"
          ]
        }
      ],
      "source": [
        "#RESNET-50 mixed augmentations\n",
        "\n",
        "resnet_aug_2 = models.resnet50(weights=models.ResNet50_Weights.DEFAULT) #Load pretrained model\n",
        "resnet_aug_2.fc = nn.Linear(resnet_aug_2.fc.in_features, num_classes) #Replace last classifier layer\n",
        "resnet_aug_2 = resnet_aug_2.to(device)\n",
        "\n",
        "optimizer = optim.SGD(resnet_aug_2.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "resnet_aug_2, resnet_history_aug_2 = train_model(\n",
        "    model=resnet_aug_2,\n",
        "    dataloaders=dataloaders_mix,\n",
        "    dataset_size=dataset_size_mix,\n",
        "    device=device,\n",
        "    criterion=criterion_weighted,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25,\n",
        "    use_mixup=False,\n",
        "    mixup_prob=0.5,\n",
        "    mixup_alpha=0.4\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "id": "fbc5ce7f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Res-net test metrics\n",
            "Accuracy:  0.7121\n",
            "Precision: 0.6996\n",
            "Recall:    0.7012\n",
            "F1-score:  0.6915\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.84      0.78      0.81        41\n",
            "       glass       0.77      0.65      0.70        51\n",
            "       metal       0.51      0.76      0.61        41\n",
            "       paper       0.87      0.75      0.80        60\n",
            "     plastic       0.79      0.67      0.73        49\n",
            "       trash       0.43      0.60      0.50        15\n",
            "\n",
            "    accuracy                           0.71       257\n",
            "   macro avg       0.70      0.70      0.69       257\n",
            "weighted avg       0.74      0.71      0.72       257\n",
            "\n",
            "Res-Net test_01 metrics\n",
            "Accuracy:  0.5039\n",
            "Precision: 0.5443\n",
            "Recall:    0.5183\n",
            "F1-score:  0.5135\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.63      0.77      0.70        31\n",
            "       glass       0.68      0.52      0.59        44\n",
            "       metal       0.33      0.56      0.41        45\n",
            "       paper       0.58      0.57      0.57        44\n",
            "     plastic       0.43      0.40      0.41        45\n",
            "       trash       0.62      0.29      0.39        45\n",
            "\n",
            "    accuracy                           0.50       254\n",
            "   macro avg       0.54      0.52      0.51       254\n",
            "weighted avg       0.54      0.50      0.50       254\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Res-net test metrics\")\n",
        "evaluate_model(resnet_aug_2, dataloaders_mix[\"test\"], device, image_datasets_mix[\"train\"].classes)\n",
        "print(\"Res-Net test_01 metrics\")\n",
        "evaluate_model(resnet_aug_2, dataloaders_mix[\"test_01\"], device, image_datasets_mix[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "801b7774",
      "metadata": {},
      "outputs": [],
      "source": [
        "del resnet\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "831df688",
      "metadata": {},
      "outputs": [],
      "source": [
        "#Efficientnet\n",
        "\n",
        "efficient_net = models.efficientnet_b0(weights=models.EfficientNet_B0_Weights.DEFAULT) #Load pretrained model\n",
        "\n",
        "in_features = efficient_net.classifier[1].in_features #Replace last classifier layer\n",
        "efficient_net.classifier[1] = nn.Linear(in_features, num_classes)\n",
        "\n",
        "efficient_net = efficient_net.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #define loss\n",
        "optimizer = optim.SGD(efficient_net.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "efficient_net, efficient_net_history = train_model(\n",
        "    model=efficient_net,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1cf9bbe5",
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"Efficient_net test metrics\")\n",
        "evaluate_model(efficient_net, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"Efficient_net test_01 metrics\")\n",
        "evaluate_model(efficient_net, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "326f52d2",
      "metadata": {},
      "outputs": [],
      "source": [
        "del efficient_net\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "80405db1",
      "metadata": {},
      "outputs": [],
      "source": [
        "#densenet\n",
        "\n",
        "densenet = models.densenet121(weights=models.DenseNet121_Weights.DEFAULT) #Load pretrained model\n",
        "densenet.classifier = nn.Linear(densenet.classifier.in_features, num_classes)\n",
        "densenet = densenet.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss() #define loss\n",
        "optimizer = optim.SGD(densenet.parameters(), lr=0.001) #define optimizer\n",
        "\n",
        "densenet, densenet_history = train_model(\n",
        "    model=densenet,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "a8cca041",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "densenet test metrics\n",
            "Accuracy:  0.9066\n",
            "Precision: 0.9172\n",
            "Recall:    0.8697\n",
            "F1-score:  0.8849\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.95      0.98      0.96        41\n",
            "       glass       0.85      0.86      0.85        51\n",
            "       metal       0.84      0.88      0.86        41\n",
            "       paper       0.97      0.98      0.98        60\n",
            "     plastic       0.90      0.92      0.91        49\n",
            "       trash       1.00      0.60      0.75        15\n",
            "\n",
            "    accuracy                           0.91       257\n",
            "   macro avg       0.92      0.87      0.88       257\n",
            "weighted avg       0.91      0.91      0.90       257\n",
            "\n",
            "densenet test_01 metrics\n",
            "Accuracy:  0.4270\n",
            "Precision: 0.8017\n",
            "Recall:    0.4294\n",
            "F1-score:  0.4185\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.24      1.00      0.39        15\n",
            "       glass       0.82      0.64      0.72        14\n",
            "       metal       1.00      0.27      0.42        15\n",
            "       paper       0.75      0.40      0.52        15\n",
            "     plastic       1.00      0.20      0.33        15\n",
            "       trash       1.00      0.07      0.12        15\n",
            "\n",
            "    accuracy                           0.43        89\n",
            "   macro avg       0.80      0.43      0.42        89\n",
            "weighted avg       0.80      0.43      0.42        89\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"densenet test metrics\")\n",
        "evaluate_model(densenet, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"densenet test_01 metrics\")\n",
        "evaluate_model(densenet, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5a4a6321",
      "metadata": {},
      "outputs": [],
      "source": [
        "del densenet\n",
        "torch.cuda.empty_cache()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "95a8bb25",
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "1.6%"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/convnext_tiny-983f1562.pth\" to /home/sera/.cache/torch/hub/checkpoints/convnext_tiny-983f1562.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100.0%\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/25\n",
            "----------\n",
            "train Loss :1.5678 Acc: 0.4255\n",
            "val Loss :1.3140 Acc: 0.5817\n",
            "\n",
            "Epoch 2/25\n",
            "----------\n",
            "train Loss :1.1305 Acc: 0.7083\n",
            "val Loss :0.9192 Acc: 0.7849\n",
            "\n",
            "Epoch 3/25\n",
            "----------\n",
            "train Loss :0.8742 Acc: 0.7940\n",
            "val Loss :0.7776 Acc: 0.8247\n",
            "\n",
            "Epoch 4/25\n",
            "----------\n",
            "train Loss :0.7217 Acc: 0.8143\n",
            "val Loss :0.6549 Acc: 0.8327\n",
            "\n",
            "Epoch 5/25\n",
            "----------\n",
            "train Loss :0.6075 Acc: 0.8455\n",
            "val Loss :0.5063 Acc: 0.8805\n",
            "\n",
            "Epoch 6/25\n",
            "----------\n",
            "train Loss :0.5324 Acc: 0.8623\n",
            "val Loss :1.1671 Acc: 0.4940\n",
            "\n",
            "Epoch 7/25\n",
            "----------\n",
            "train Loss :0.5099 Acc: 0.8613\n",
            "val Loss :0.4446 Acc: 0.8805\n",
            "\n",
            "Epoch 8/25\n",
            "----------\n",
            "train Loss :0.4330 Acc: 0.8920\n",
            "val Loss :0.4732 Acc: 0.8845\n",
            "\n",
            "Epoch 9/25\n",
            "----------\n",
            "train Loss :0.3919 Acc: 0.9000\n",
            "val Loss :0.3862 Acc: 0.8964\n",
            "\n",
            "Epoch 10/25\n",
            "----------\n",
            "train Loss :0.3516 Acc: 0.9099\n",
            "val Loss :0.5459 Acc: 0.8406\n",
            "\n",
            "Epoch 11/25\n",
            "----------\n",
            "train Loss :0.3484 Acc: 0.9104\n",
            "val Loss :0.2957 Acc: 0.9323\n",
            "\n",
            "Epoch 12/25\n",
            "----------\n",
            "train Loss :0.2991 Acc: 0.9292\n",
            "val Loss :0.5274 Acc: 0.8247\n",
            "\n",
            "Epoch 13/25\n",
            "----------\n",
            "train Loss :0.2851 Acc: 0.9247\n",
            "val Loss :0.2701 Acc: 0.9243\n",
            "\n",
            "Epoch 14/25\n",
            "----------\n",
            "train Loss :0.2624 Acc: 0.9336\n",
            "val Loss :0.2840 Acc: 0.9203\n",
            "\n",
            "Epoch 15/25\n",
            "----------\n",
            "train Loss :0.2420 Acc: 0.9376\n",
            "val Loss :0.2537 Acc: 0.9243\n",
            "\n",
            "Epoch 16/25\n",
            "----------\n",
            "train Loss :0.2219 Acc: 0.9539\n",
            "val Loss :0.4267 Acc: 0.8685\n",
            "\n",
            "Epoch 17/25\n",
            "----------\n",
            "train Loss :0.2118 Acc: 0.9485\n",
            "val Loss :0.2438 Acc: 0.9243\n",
            "\n",
            "Epoch 18/25\n",
            "----------\n",
            "train Loss :0.1969 Acc: 0.9554\n",
            "val Loss :0.2688 Acc: 0.9124\n",
            "\n",
            "Epoch 19/25\n",
            "----------\n",
            "train Loss :0.1854 Acc: 0.9574\n",
            "val Loss :0.2439 Acc: 0.9124\n",
            "\n",
            "Epoch 20/25\n",
            "----------\n",
            "train Loss :0.1709 Acc: 0.9663\n",
            "val Loss :0.2548 Acc: 0.9163\n",
            "\n",
            "Epoch 21/25\n",
            "----------\n",
            "train Loss :0.1546 Acc: 0.9673\n",
            "val Loss :0.2191 Acc: 0.9402\n",
            "\n",
            "Epoch 22/25\n",
            "----------\n",
            "train Loss :0.1474 Acc: 0.9688\n",
            "val Loss :0.2188 Acc: 0.9283\n",
            "\n",
            "Epoch 23/25\n",
            "----------\n",
            "train Loss :0.1335 Acc: 0.9733\n",
            "val Loss :0.2186 Acc: 0.9203\n",
            "\n",
            "Epoch 24/25\n",
            "----------\n",
            "train Loss :0.1223 Acc: 0.9807\n",
            "val Loss :0.2335 Acc: 0.9203\n",
            "\n",
            "Epoch 25/25\n",
            "----------\n",
            "train Loss :0.1242 Acc: 0.9767\n",
            "val Loss :0.1869 Acc: 0.9482\n",
            "\n",
            "Training complete in 7m 27s\n",
            "Best val Acc: 0.9482\n"
          ]
        }
      ],
      "source": [
        "# ConvNeXt-Base\n",
        "convnext_base = models.convnext_tiny(weights=models.ConvNeXt_Tiny_Weights.IMAGENET1K_V1)\n",
        "in_features = convnext_base.classifier[2].in_features\n",
        "convnext_base.classifier[2] = nn.Linear(in_features, num_classes)\n",
        "convnext_base = convnext_base.to(device)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(convnext_base.parameters(), lr=0.001)  \n",
        "\n",
        "convnext_base, convnext_base_history = train_model(\n",
        "    model=convnext_base,\n",
        "    dataloaders=dataloaders,\n",
        "    dataset_size=dataset_size,\n",
        "    device=device,\n",
        "    criterion=criterion,\n",
        "    optimizer=optimizer,\n",
        "    num_epochs=25\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "0be71614",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ConvNext test metrics\n",
            "Accuracy:  0.9183\n",
            "Precision: 0.9298\n",
            "Recall:    0.8988\n",
            "F1-score:  0.9091\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.95      0.98      0.96        41\n",
            "       glass       0.95      0.82      0.88        51\n",
            "       metal       0.87      0.95      0.91        41\n",
            "       paper       0.97      0.95      0.96        60\n",
            "     plastic       0.84      0.96      0.90        49\n",
            "       trash       1.00      0.73      0.85        15\n",
            "\n",
            "    accuracy                           0.92       257\n",
            "   macro avg       0.93      0.90      0.91       257\n",
            "weighted avg       0.92      0.92      0.92       257\n",
            "\n",
            "convnext_base test_01 metrics\n",
            "Accuracy:  0.3708\n",
            "Precision: 0.5576\n",
            "Recall:    0.3698\n",
            "F1-score:  0.3517\n",
            "\n",
            "Per-class breakdown:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   cardboard       0.23      1.00      0.38        15\n",
            "       glass       1.00      0.29      0.44        14\n",
            "       metal       0.83      0.33      0.48        15\n",
            "       paper       0.83      0.33      0.48        15\n",
            "     plastic       0.44      0.27      0.33        15\n",
            "       trash       0.00      0.00      0.00        15\n",
            "\n",
            "    accuracy                           0.37        89\n",
            "   macro avg       0.56      0.37      0.35        89\n",
            "weighted avg       0.55      0.37      0.35        89\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"ConvNext test metrics\")\n",
        "evaluate_model(convnext_base, dataloaders[\"test\"], device, image_datasets[\"train\"].classes)\n",
        "print(\"convnext_base test_01 metrics\")\n",
        "evaluate_model(convnext_base, dataloaders[\"test_01\"], device, image_datasets[\"train\"].classes)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13a904d6",
      "metadata": {},
      "outputs": [],
      "source": [
        "del convnext_base\n",
        "torch.cuda.empty_cache()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (thesis venv)",
      "language": "python",
      "name": "thesis-venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
